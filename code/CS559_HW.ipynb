{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS559_HW.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"vZsmPAsO5Sqf","colab_type":"code","outputId":"37c060aa-a013-484b-d494-61d02f9603d4","executionInfo":{"status":"ok","timestamp":1554661623725,"user_tz":-180,"elapsed":610,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"VyrGQWpO5UDN","colab_type":"code","outputId":"395322c1-bf02-42d4-ce57-c1b0da361ada","executionInfo":{"status":"ok","timestamp":1554661625694,"user_tz":-180,"elapsed":2309,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["cd /content/drive/'My Drive'/'Colab Notebooks'/CS559_HW"],"execution_count":41,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/CS559_HW\n"],"name":"stdout"}]},{"metadata":{"id":"chdX_5e7dp_3","colab_type":"code","colab":{}},"cell_type":"code","source":["from scipy.signal import savgol_filter\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import os\n","import sys\n","\n","\n","images_train = np.load('./data/images_train.npy')\n","labels_train = np.load('./data/labels_train.npy')\n","images_validation = np.load('./data/images_validation.npy')\n","labels_validation = np.load('./data/labels_validation.npy')\n","images_test = np.load('./data/images_test.npy')\n","labels_test = np.load('./data/labels_test.npy')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6h-uj8t55UGj","colab_type":"code","colab":{}},"cell_type":"code","source":["def model(learning_rate = 0.001, dr_keep=0.5, l2_scale=0.1, filter_coef=1):\n","  x = tf.placeholder(tf.float32, [None, 91*91], name=\"features_placeholder\")\n","  x_shaped = tf.reshape(x, [-1, 91, 91, 1])\n","  y = tf.placeholder(tf.int32, [None, 1], name=\"labels_placeholder\")\n","  train_mode = tf.placeholder(tf.bool, name=\"train_mode_placeholder\")\n","\n","  \n","  conv1 = tf.contrib.layers.conv2d(\n","                                   inputs=x_shaped,\n","                                   num_outputs=24*filter_coef,\n","                                   kernel_size=5,\n","                                   stride=2,\n","                                   padding='VALID',\n","                                   # activation_fn=tf.nn.relu,\n","                                   # weights_initializer=w_initializer,\n","                                   weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                   # biases_initializer=tf.zeros_initializer(),\n","                                   # name = \"Conv2d_1\"\n","                                  )\n","  \n","  bn1 = tf.layers.batch_normalization(\n","                                      inputs=conv1,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name = \"BatchNorm_1\"\n","                                     )\n","  \n","  conv2 = tf.contrib.layers.conv2d(\n","                                   inputs=bn1,\n","                                   num_outputs=36*filter_coef,\n","                                   kernel_size=5,\n","                                   stride=2,\n","                                   padding='VALID',\n","                                   # activation_fn=tf.nn.relu,\n","                                   # weights_initializer=w_initializer,\n","                                   weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                   # biases_initializer=tf.zeros_initializer(),\n","                                   # name = \"Conv2d_2\"\n","                                  )\n","  \n","  conv3 = tf.contrib.layers.conv2d(\n","                                   inputs=conv2,\n","                                   num_outputs=48*filter_coef,\n","                                   kernel_size=5,\n","                                   stride=2,\n","                                   padding='VALID',\n","                                   # activation_fn=tf.nn.relu,\n","                                   # weights_initializer=w_initializer,\n","                                   weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                   # biases_initializer=tf.zeros_initializer(),\n","                                   # name = \"Conv2d_3\"\n","                                  )\n","  \n","  bn2 = tf.layers.batch_normalization(\n","                                      inputs=conv3,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name = \"BatchNorm_2\"\n","                                     )\n","\n","  conv4 = tf.contrib.layers.conv2d(\n","                                   inputs=bn2,\n","                                   num_outputs=64*filter_coef,\n","                                   kernel_size=3,\n","                                   stride=1,\n","                                   padding='VALID',\n","                                   # activation_fn=tf.nn.relu,\n","                                   # weights_initializer=w_initializer,\n","                                   weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                   # biases_initializer=tf.zeros_initializer(),\n","                                   # name = \"Conv2d_4\"\n","                                  )\n","  \n","  conv5 = tf.contrib.layers.conv2d(\n","                                   inputs=conv4,\n","                                   num_outputs=64*filter_coef,\n","                                   kernel_size=3,\n","                                   stride=1,\n","                                   padding='VALID',\n","                                   # activation_fn=tf.nn.relu,\n","                                   # weights_initializer=w_initializer,\n","                                   weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                   # biases_initializer=tf.zeros_initializer(),\n","                                   # name = \"Conv2d_5\"\n","                                  )\n","  \n","    \n","  bn3 = tf.layers.batch_normalization(\n","                                      inputs=conv5,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name = \"BatchNorm_3\"\n","                                     )\n","                                     \n","  flattened = tf.contrib.layers.flatten(\n","                                        inputs=bn3,\n","                                        # name=\"Flatten\"\n","                                       )\n","\n","  \n","  dr1 = tf.contrib.layers.dropout(\n","                                  inputs=flattened,\n","                                  keep_prob=dr_keep,\n","                                  is_training=train_mode,\n","                                  # name=\"Dropout_1\"\n","                                 )\n","  \n","  fc1 = tf.contrib.layers.fully_connected(\n","                                          inputs=dr1,\n","                                          num_outputs=1164,\n","                                          # activation_fn=tf.nn.relu,\n","                                          # weights_initializer=w_initializer,\n","                                          weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                          # biases_initializer=tf.zeros_initializer(),\n","                                          # name=\"FullyConnected_1\"\n","                                         )\n","                                     \n","  bn4 = tf.layers.batch_normalization(\n","                                      inputs=fc1,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name=\"BatchNorm_4\"\n","                                     )\n","      \n","  dr2 = tf.contrib.layers.dropout(\n","                                  inputs=bn4,\n","                                  keep_prob=dr_keep,\n","                                  is_training=train_mode,\n","                                  # name=\"Dropout_2\"\n","                                 )\n","  \n","  fc2 = tf.contrib.layers.fully_connected(\n","                                          inputs=dr2,\n","                                          num_outputs=100,\n","                                          # activation_fn=tf.nn.relu,\n","                                          # weights_initializer=w_initializer,\n","                                          weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                          # biases_initializer=tf.zeros_initializer(),\n","                                          # name=\"FullyConnected_2\"\n","                                         )\n","                                     \n","    \n","  bn5 = tf.layers.batch_normalization(fc2,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name=\"BatchNorm_5\"\n","                                     )\n","      \n","  dr3 = tf.contrib.layers.dropout(\n","                                  inputs=bn5,\n","                                  keep_prob=dr_keep,\n","                                  is_training=train_mode,\n","                                  # name=\"Dropout_3\"\n","                                 )\n","  \n","  fc3 = tf.contrib.layers.fully_connected(\n","                                          inputs=dr3,\n","                                          num_outputs=50,\n","                                          # activation_fn=tf.nn.relu,\n","                                          # weights_initializer=w_initializer,\n","                                          weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                          # biases_initializer=tf.zeros_initializer(),\n","                                          # name=\"FullyConnected_3\"\n","                                         )\n","                                     \n","    \n","  bn6 = tf.layers.batch_normalization(fc3,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name=\"BatchNorm_6\"\n","                                     )\n","  \n","  dr4 = tf.contrib.layers.dropout(\n","                                  inputs=bn6,\n","                                  keep_prob=dr_keep,\n","                                  is_training=train_mode,\n","                                  # name=\"Dropout_4\"\n","                                 )\n","  \n","  fc4 = tf.contrib.layers.fully_connected(\n","                                          inputs=dr4,\n","                                          num_outputs=10,\n","                                          # activation_fn=tf.nn.relu,\n","                                          # weights_initializer=w_initializer,\n","                                          weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                          # biases_initializer=tf.zeros_initializer(),\n","                                          # name=\"FullyConnected_4\"\n","                                         )\n","    \n","  bn7 = tf.layers.batch_normalization(\n","                                      inputs=fc4,\n","                                      training=train_mode,\n","                                      momentum=0.9,\n","                                      # name=\"BatchNorm_7\"\n","                                     )\n","      \n","  dr5 = tf.contrib.layers.dropout(\n","                                  inputs=bn7,\n","                                  keep_prob=dr_keep,\n","                                  is_training=train_mode,\n","                                  # name=\"Dropout_5\"\n","                                 )\n","  \n","  fc5 = tf.contrib.layers.fully_connected(\n","                                          inputs=dr5,\n","                                          num_outputs=1,\n","                                          # activation_fn=tf.nn.relu,\n","                                          # weights_initializer=w_initializer,\n","                                          weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_scale),\n","                                          # biases_initializer=tf.zeros_initializer(),\n","                                          # name=\"FullyConnected_5\"\n","                                         )\n","  # For real output round and clip\n","  output = tf.round(fc5)\n","  output = tf.cast(output, tf.int32)\n","  output = tf.clip_by_value(output, 1, 80)\n","  \n","  # MAE for evaluating model \n","  loss_mae_round = tf.losses.absolute_difference(y, output)  \n","  \n","  # Loss function for optimizer\n","  loss_mae = tf.losses.mean_squared_error(y, fc5)\n","  \n","  # L2 Regularization\n","  reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","  loss_mae += sum(reg_losses)\n","\n","  # Adam Optimizer\n","  optimizer = tf.contrib.optimizer_v2.AdamOptimizer(learning_rate=learning_rate)\n","  \n","  # For BatchNorm, the moving_mean and moving_variance need to be updated while training\n","  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","  \n","  # Optimize and group\n","  train_op = optimizer.minimize(loss_mae)  \n","  train_op = tf.group([train_op, update_ops])\n","  \n","  return x, y, output, train_mode, train_op, loss_mae_round"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q8LRtd-tVpc8","colab_type":"code","colab":{}},"cell_type":"code","source":["def restore_model(model_name=\"model\", learning_rate= 0.0001, dr_keep=0.6, l2_scale=0.01, filter_coef=2):\n","  tf.reset_default_graph()\n","  x, y, output, train_mode, train_op, loss_mae_round = model(learning_rate=learning_rate, dr_keep=dr_keep, l2_scale=l2_scale, filter_coef=filter_coef)\n","\n","  log_directory = \"./logs/\" + model_name + \"/\"\n","  \n","  saver = tf.train.Saver()\n","\n","  with tf.Session() as sess:\n","    saver.restore(sess, log_directory + model_name + \".ckpt\")\n","    print(model_name, \"restored.\")\n","\n","    validation_loss = sess.run(loss_mae_round, feed_dict={x: images_validation, y: labels_validation, train_mode:False})\n","    test_loss = sess.run(loss_mae_round, feed_dict={x: images_test, y: labels_test, train_mode:False})\n","    \n","  return validation_loss, test_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GPTE9-v0sO1b","colab_type":"code","colab":{}},"cell_type":"code","source":["def run_model(learning_rate = 0.001, epochs = 100, batch_size = 50, dr_keep=0.5, l2_scale=0.1, filter_coef=1, early_stop = 10, save_result=True, outfile_name=None):\n","  global images_train\n","  global images_validation\n","  global images_test\n","  \n","  if not outfile_name:\n","    outfile_name = \"model_lr\" + str(learning_rate) + \"_bs\" + str(batch_size) + \"_drk\" + str(dr_keep) + \"_l2\" + str(l2_scale) + \"_fc\" + str(filter_coef)\n","  print(\"\\nStarting \" + outfile_name)\n","  log_directory = \"./logs/\" + outfile_name + \"/\"\n","  \n","  if not os.path.exists(log_directory):\n","    os.makedirs(log_directory)\n","  \n","  tf.reset_default_graph()\n","  x, y, output, train_mode, train_op, loss_mae_round = model(learning_rate, dr_keep, l2_scale, filter_coef)\n","  \n","  train_total_batch = int(len(labels_train) / batch_size)\n","\n","  # setup the initialisation operator\n","  init_op = tf.global_variables_initializer()\n","  saver = None\n","  if save_result:\n","    saver = tf.train.Saver()\n","    \n","  with tf.Session() as sess:\n","    \"\"\" # Image standardization\n","    with sess.as_default():\n","      images_train = tf.image.per_image_standardization(images_train.reshape([-1, 91,91,1])).eval()\n","      images_validation = tf.image.per_image_standardization(images_validation.reshape([-1, 91,91,1])).eval()\n","      images_test = tf.image.per_image_standardization(images_test.reshape([-1, 91,91,1])).eval()\n","    \"\"\"\n","    # initialize the variables\n","    sess.run(init_op)\n","    \n","    train_losses = []\n","    validation_losses = []\n","    \n","    best_validation_loss = np.inf\n","    best_validation_loss_epoch = 0\n","\n","    early_stop_counter = 0\n","    \n","    for epoch in range(epochs):\n","      sum_loss = 0\n","      for i in range(train_total_batch):\n","        batch_x = images_train[i*batch_size:i*batch_size+batch_size]\n","        batch_y = labels_train[i*batch_size:i*batch_size+batch_size]\n","        _, c= sess.run([train_op, loss_mae_round], feed_dict={x: batch_x, y: batch_y, train_mode:True})\n","        sum_loss += c\n","\n","      train_avg_loss = sum_loss / train_total_batch\n","\n","      # Validation loss after the epoch\n","      validation_loss = sess.run(loss_mae_round, feed_dict={x: images_validation, y: labels_validation, train_mode:False})\n","\n","      train_losses.append(train_avg_loss)\n","      validation_losses.append(validation_loss)\n","\n","      print(\"Epoch:\", (epoch + 1), \", train loss:\", \"{:.3f}\".format(train_avg_loss), \", validation loss: {:.3f}\".format(validation_loss))\n","\n","      if validation_loss < best_validation_loss:\n","        early_stop_counter = 0\n","        best_validation_loss = validation_loss\n","        best_validation_loss_epoch = epoch + 1\n","        if save_result:\n","          save_path = saver.save(sess, log_directory + outfile_name + \".ckpt\")\n","          print(\"Model saved in path: %s\" % save_path)\n","      else:\n","        early_stop_counter += 1\n","\n","      if early_stop_counter >= early_stop:\n","        print(\"Early stopping is trigger at step: {} loss:{}\".format(epoch + 1, best_validation_loss))\n","        break\n","    if not save_result:\n","      test_loss = sess.run(loss_mae_round, feed_dict={x: images_test, y: labels_test, train_mode:False})\n","      print(\"\\nTraining complete! Test loss: {:.3f}\".format(test_loss))\n","\n","  if save_result:\n","    validation_loss, test_loss = restore_model(outfile_name, learning_rate=learning_rate, dr_keep=dr_keep, l2_scale=l2_scale, filter_coef=filter_coef)\n","\n","    with open(\"./results/\" + outfile_name + '.txt', 'w') as f:\n","      f.write(\"Epoch Train_Loss Validation_Loss\\n\")\n","      for i in range(len(train_losses)):\n","        f.write(\"{0} {1:.3f} {2:.3f}\\n\".format(i + 1, train_losses[i], validation_losses[i]))\n","      f.write(\"Training complete! Best validation at epoch {}. Validation loss: {:.3f}, Test loss: {:.3f}\".format(best_validation_loss_epoch, validation_loss, test_loss))\n","\n","    print(\"\\nTraining complete! Best validation at epoch {}. Validation loss: {:.3f}, Test loss: {:.3f}\".format(best_validation_loss_epoch, validation_loss, test_loss))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1Hgv_sGhuZ2O","colab_type":"code","colab":{}},"cell_type":"code","source":["def graph_tensorboard():   \n","  tf.reset_default_graph()\n","  x, y, output, train_mode, train_op, loss_mae_round = model(learning_rate=0.01, dr_keep=0.6, l2_scale=0.1, filter_coef=2)\n","\n","  init_op = tf.global_variables_initializer()\n","\n","  with tf.Session() as sess:\n","      sess.run(init_op)\n","      writer = tf.summary.FileWriter(\"output\", sess.graph)\n","      print(sess.run([train_op, loss_mae_round], feed_dict={x: images_test, y: labels_test, train_mode:True}))\n","      writer.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4uMlc9ow-py9","colab_type":"code","outputId":"ee98c9dc-f35f-4e8f-be97-d8043256b7d9","executionInfo":{"status":"ok","timestamp":1554668853234,"user_tz":-180,"elapsed":7199729,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}},"colab":{"base_uri":"https://localhost:8080/","height":40958}},"cell_type":"code","source":["run_model(learning_rate=0.0001, epochs=3000, batch_size=200, dr_keep=0.6, l2_scale=0.01, filter_coef=2, early_stop=500, save_result=True, outfile_name=\"LastVersion\")"],"execution_count":48,"outputs":[{"output_type":"stream","text":["\n","Starting LastVersion\n","Epoch: 1 , train loss: 30.735 , validation loss: 30.078\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 2 , train loss: 30.629 , validation loss: 29.753\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 3 , train loss: 30.517 , validation loss: 30.297\n","Epoch: 4 , train loss: 30.394 , validation loss: 30.238\n","Epoch: 5 , train loss: 30.296 , validation loss: 30.214\n","Epoch: 6 , train loss: 30.163 , validation loss: 30.074\n","Epoch: 7 , train loss: 30.086 , validation loss: 29.991\n","Epoch: 8 , train loss: 30.000 , validation loss: 29.928\n","Epoch: 9 , train loss: 29.929 , validation loss: 29.870\n","Epoch: 10 , train loss: 29.885 , validation loss: 29.841\n","Epoch: 11 , train loss: 29.800 , validation loss: 29.739\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 12 , train loss: 29.747 , validation loss: 29.727\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 13 , train loss: 29.684 , validation loss: 29.694\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 14 , train loss: 29.648 , validation loss: 29.611\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 15 , train loss: 29.617 , validation loss: 29.556\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 16 , train loss: 29.558 , validation loss: 29.525\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 17 , train loss: 29.513 , validation loss: 29.470\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 18 , train loss: 29.450 , validation loss: 29.468\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 19 , train loss: 29.427 , validation loss: 29.441\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 20 , train loss: 29.384 , validation loss: 29.347\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 21 , train loss: 29.336 , validation loss: 29.340\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 22 , train loss: 29.294 , validation loss: 29.244\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 23 , train loss: 29.239 , validation loss: 29.186\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 24 , train loss: 29.200 , validation loss: 29.042\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 25 , train loss: 29.173 , validation loss: 29.127\n","Epoch: 26 , train loss: 29.121 , validation loss: 29.089\n","Epoch: 27 , train loss: 29.097 , validation loss: 29.041\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 28 , train loss: 29.087 , validation loss: 29.165\n","Epoch: 29 , train loss: 29.015 , validation loss: 28.921\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 30 , train loss: 29.024 , validation loss: 29.232\n","Epoch: 31 , train loss: 28.934 , validation loss: 28.728\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 32 , train loss: 28.889 , validation loss: 29.037\n","Epoch: 33 , train loss: 28.838 , validation loss: 29.053\n","Epoch: 34 , train loss: 28.790 , validation loss: 28.732\n","Epoch: 35 , train loss: 28.774 , validation loss: 29.089\n","Epoch: 36 , train loss: 28.739 , validation loss: 28.388\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 37 , train loss: 28.657 , validation loss: 28.802\n","Epoch: 38 , train loss: 28.605 , validation loss: 28.530\n","Epoch: 39 , train loss: 28.566 , validation loss: 29.283\n","Epoch: 40 , train loss: 28.521 , validation loss: 28.585\n","Epoch: 41 , train loss: 28.459 , validation loss: 28.410\n","Epoch: 42 , train loss: 28.420 , validation loss: 27.263\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 43 , train loss: 28.345 , validation loss: 28.704\n","Epoch: 44 , train loss: 28.347 , validation loss: 28.165\n","Epoch: 45 , train loss: 28.237 , validation loss: 28.494\n","Epoch: 46 , train loss: 28.292 , validation loss: 28.217\n","Epoch: 47 , train loss: 28.192 , validation loss: 28.363\n","Epoch: 48 , train loss: 28.175 , validation loss: 28.393\n","Epoch: 49 , train loss: 28.096 , validation loss: 28.085\n","Epoch: 50 , train loss: 28.026 , validation loss: 27.673\n","Epoch: 51 , train loss: 27.993 , validation loss: 28.935\n","Epoch: 52 , train loss: 27.960 , validation loss: 28.124\n","Epoch: 53 , train loss: 27.916 , validation loss: 28.415\n","Epoch: 54 , train loss: 27.888 , validation loss: 28.070\n","Epoch: 55 , train loss: 27.836 , validation loss: 28.778\n","Epoch: 56 , train loss: 27.796 , validation loss: 27.898\n","Epoch: 57 , train loss: 27.736 , validation loss: 27.821\n","Epoch: 58 , train loss: 27.740 , validation loss: 27.356\n","Epoch: 59 , train loss: 27.711 , validation loss: 27.536\n","Epoch: 60 , train loss: 27.633 , validation loss: 27.786\n","Epoch: 61 , train loss: 27.615 , validation loss: 27.442\n","Epoch: 62 , train loss: 27.546 , validation loss: 27.526\n","Epoch: 63 , train loss: 27.521 , validation loss: 27.762\n","Epoch: 64 , train loss: 27.528 , validation loss: 28.055\n","Epoch: 65 , train loss: 27.406 , validation loss: 27.694\n","Epoch: 66 , train loss: 27.395 , validation loss: 27.362\n","Epoch: 67 , train loss: 27.354 , validation loss: 27.510\n","Epoch: 68 , train loss: 27.299 , validation loss: 27.351\n","Epoch: 69 , train loss: 27.238 , validation loss: 27.431\n","Epoch: 70 , train loss: 27.193 , validation loss: 27.144\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 71 , train loss: 27.205 , validation loss: 27.960\n","Epoch: 72 , train loss: 27.136 , validation loss: 27.524\n","Epoch: 73 , train loss: 27.082 , validation loss: 27.359\n","Epoch: 74 , train loss: 27.068 , validation loss: 27.373\n","Epoch: 75 , train loss: 26.996 , validation loss: 27.044\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 76 , train loss: 26.942 , validation loss: 27.409\n","Epoch: 77 , train loss: 26.928 , validation loss: 27.265\n","Epoch: 78 , train loss: 26.916 , validation loss: 27.394\n","Epoch: 79 , train loss: 26.874 , validation loss: 26.900\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 80 , train loss: 26.831 , validation loss: 27.601\n","Epoch: 81 , train loss: 26.756 , validation loss: 27.091\n","Epoch: 82 , train loss: 26.718 , validation loss: 26.749\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 83 , train loss: 26.681 , validation loss: 27.241\n","Epoch: 84 , train loss: 26.621 , validation loss: 26.753\n","Epoch: 85 , train loss: 26.548 , validation loss: 26.810\n","Epoch: 86 , train loss: 26.502 , validation loss: 26.661\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 87 , train loss: 26.503 , validation loss: 26.720\n","Epoch: 88 , train loss: 26.419 , validation loss: 26.573\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 89 , train loss: 26.403 , validation loss: 27.113\n","Epoch: 90 , train loss: 26.361 , validation loss: 26.220\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 91 , train loss: 26.361 , validation loss: 27.215\n","Epoch: 92 , train loss: 26.343 , validation loss: 25.436\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 93 , train loss: 26.216 , validation loss: 26.981\n","Epoch: 94 , train loss: 26.219 , validation loss: 26.476\n","Epoch: 95 , train loss: 26.171 , validation loss: 27.364\n","Epoch: 96 , train loss: 26.147 , validation loss: 25.971\n","Epoch: 97 , train loss: 26.070 , validation loss: 26.468\n","Epoch: 98 , train loss: 26.052 , validation loss: 25.724\n","Epoch: 99 , train loss: 26.033 , validation loss: 26.927\n","Epoch: 100 , train loss: 25.962 , validation loss: 27.063\n","Epoch: 101 , train loss: 25.894 , validation loss: 26.202\n","Epoch: 102 , train loss: 25.867 , validation loss: 25.908\n","Epoch: 103 , train loss: 25.830 , validation loss: 26.183\n","Epoch: 104 , train loss: 25.737 , validation loss: 26.653\n","Epoch: 105 , train loss: 25.716 , validation loss: 26.114\n","Epoch: 106 , train loss: 25.651 , validation loss: 25.406\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 107 , train loss: 25.672 , validation loss: 26.540\n","Epoch: 108 , train loss: 25.583 , validation loss: 25.645\n","Epoch: 109 , train loss: 25.544 , validation loss: 26.262\n","Epoch: 110 , train loss: 25.458 , validation loss: 25.549\n","Epoch: 111 , train loss: 25.505 , validation loss: 26.448\n","Epoch: 112 , train loss: 25.442 , validation loss: 25.899\n","Epoch: 113 , train loss: 25.340 , validation loss: 25.278\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 114 , train loss: 25.374 , validation loss: 26.414\n","Epoch: 115 , train loss: 25.338 , validation loss: 25.348\n","Epoch: 116 , train loss: 25.262 , validation loss: 26.070\n","Epoch: 117 , train loss: 25.190 , validation loss: 25.816\n","Epoch: 118 , train loss: 25.149 , validation loss: 24.866\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 119 , train loss: 25.132 , validation loss: 26.029\n","Epoch: 120 , train loss: 25.053 , validation loss: 25.406\n","Epoch: 121 , train loss: 25.025 , validation loss: 25.638\n","Epoch: 122 , train loss: 24.937 , validation loss: 25.124\n","Epoch: 123 , train loss: 24.899 , validation loss: 25.886\n","Epoch: 124 , train loss: 24.919 , validation loss: 25.268\n","Epoch: 125 , train loss: 24.877 , validation loss: 25.071\n","Epoch: 126 , train loss: 24.750 , validation loss: 25.780\n","Epoch: 127 , train loss: 24.749 , validation loss: 24.510\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 128 , train loss: 24.688 , validation loss: 24.848\n","Epoch: 129 , train loss: 24.675 , validation loss: 25.116\n","Epoch: 130 , train loss: 24.624 , validation loss: 24.886\n","Epoch: 131 , train loss: 24.588 , validation loss: 24.999\n","Epoch: 132 , train loss: 24.466 , validation loss: 25.269\n","Epoch: 133 , train loss: 24.540 , validation loss: 24.555\n","Epoch: 134 , train loss: 24.488 , validation loss: 24.721\n","Epoch: 135 , train loss: 24.362 , validation loss: 24.870\n","Epoch: 136 , train loss: 24.371 , validation loss: 24.965\n","Epoch: 137 , train loss: 24.322 , validation loss: 24.545\n","Epoch: 138 , train loss: 24.211 , validation loss: 24.351\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 139 , train loss: 24.199 , validation loss: 25.292\n","Epoch: 140 , train loss: 24.128 , validation loss: 24.436\n","Epoch: 141 , train loss: 24.102 , validation loss: 23.890\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 142 , train loss: 24.051 , validation loss: 24.218\n","Epoch: 143 , train loss: 23.979 , validation loss: 25.274\n","Epoch: 144 , train loss: 23.956 , validation loss: 24.093\n","Epoch: 145 , train loss: 23.897 , validation loss: 24.434\n","Epoch: 146 , train loss: 23.917 , validation loss: 24.412\n","Epoch: 147 , train loss: 23.822 , validation loss: 23.909\n","Epoch: 148 , train loss: 23.706 , validation loss: 24.324\n","Epoch: 149 , train loss: 23.647 , validation loss: 24.371\n","Epoch: 150 , train loss: 23.641 , validation loss: 24.733\n","Epoch: 151 , train loss: 23.544 , validation loss: 23.555\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 152 , train loss: 23.581 , validation loss: 24.090\n","Epoch: 153 , train loss: 23.501 , validation loss: 23.961\n","Epoch: 154 , train loss: 23.397 , validation loss: 24.696\n","Epoch: 155 , train loss: 23.314 , validation loss: 24.554\n","Epoch: 156 , train loss: 23.219 , validation loss: 23.834\n","Epoch: 157 , train loss: 23.233 , validation loss: 23.558\n","Epoch: 158 , train loss: 23.294 , validation loss: 24.248\n","Epoch: 159 , train loss: 23.157 , validation loss: 24.157\n","Epoch: 160 , train loss: 23.107 , validation loss: 23.480\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 161 , train loss: 23.098 , validation loss: 23.975\n","Epoch: 162 , train loss: 23.039 , validation loss: 23.510\n","Epoch: 163 , train loss: 22.972 , validation loss: 23.765\n","Epoch: 164 , train loss: 22.881 , validation loss: 23.815\n","Epoch: 165 , train loss: 22.966 , validation loss: 23.722\n","Epoch: 166 , train loss: 22.771 , validation loss: 23.575\n","Epoch: 167 , train loss: 22.798 , validation loss: 23.872\n","Epoch: 168 , train loss: 22.804 , validation loss: 23.747\n","Epoch: 169 , train loss: 22.644 , validation loss: 23.403\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 170 , train loss: 22.591 , validation loss: 23.133\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 171 , train loss: 22.611 , validation loss: 23.350\n","Epoch: 172 , train loss: 22.576 , validation loss: 23.565\n","Epoch: 173 , train loss: 22.358 , validation loss: 23.362\n","Epoch: 174 , train loss: 22.306 , validation loss: 23.321\n","Epoch: 175 , train loss: 22.323 , validation loss: 22.759\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 176 , train loss: 22.177 , validation loss: 23.229\n","Epoch: 177 , train loss: 22.179 , validation loss: 22.961\n","Epoch: 178 , train loss: 22.137 , validation loss: 22.478\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 179 , train loss: 22.172 , validation loss: 23.199\n","Epoch: 180 , train loss: 22.029 , validation loss: 22.975\n","Epoch: 181 , train loss: 21.978 , validation loss: 22.099\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 182 , train loss: 21.912 , validation loss: 22.642\n","Epoch: 183 , train loss: 21.831 , validation loss: 22.916\n","Epoch: 184 , train loss: 21.729 , validation loss: 22.229\n","Epoch: 185 , train loss: 21.702 , validation loss: 22.556\n","Epoch: 186 , train loss: 21.629 , validation loss: 22.039\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 187 , train loss: 21.708 , validation loss: 22.519\n","Epoch: 188 , train loss: 21.549 , validation loss: 22.523\n","Epoch: 189 , train loss: 21.529 , validation loss: 22.071\n","Epoch: 190 , train loss: 21.418 , validation loss: 22.132\n","Epoch: 191 , train loss: 21.442 , validation loss: 22.761\n","Epoch: 192 , train loss: 21.400 , validation loss: 22.082\n","Epoch: 193 , train loss: 21.171 , validation loss: 21.475\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 194 , train loss: 21.155 , validation loss: 21.732\n","Epoch: 195 , train loss: 21.069 , validation loss: 21.568\n","Epoch: 196 , train loss: 21.123 , validation loss: 21.596\n","Epoch: 197 , train loss: 21.050 , validation loss: 21.454\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 198 , train loss: 20.932 , validation loss: 21.969\n","Epoch: 199 , train loss: 20.926 , validation loss: 21.657\n","Epoch: 200 , train loss: 20.806 , validation loss: 21.277\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 201 , train loss: 20.705 , validation loss: 21.046\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 202 , train loss: 20.831 , validation loss: 21.999\n","Epoch: 203 , train loss: 20.678 , validation loss: 22.218\n","Epoch: 204 , train loss: 20.573 , validation loss: 21.276\n","Epoch: 205 , train loss: 20.531 , validation loss: 21.284\n","Epoch: 206 , train loss: 20.460 , validation loss: 21.261\n","Epoch: 207 , train loss: 20.334 , validation loss: 20.850\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 208 , train loss: 20.389 , validation loss: 20.898\n","Epoch: 209 , train loss: 20.239 , validation loss: 21.327\n","Epoch: 210 , train loss: 20.259 , validation loss: 21.184\n","Epoch: 211 , train loss: 20.119 , validation loss: 20.729\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 212 , train loss: 20.210 , validation loss: 21.079\n","Epoch: 213 , train loss: 20.043 , validation loss: 20.535\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 214 , train loss: 19.940 , validation loss: 20.958\n","Epoch: 215 , train loss: 19.878 , validation loss: 21.317\n","Epoch: 216 , train loss: 19.890 , validation loss: 19.844\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 217 , train loss: 19.707 , validation loss: 21.114\n","Epoch: 218 , train loss: 19.680 , validation loss: 20.365\n","Epoch: 219 , train loss: 19.826 , validation loss: 20.739\n","Epoch: 220 , train loss: 19.686 , validation loss: 20.312\n","Epoch: 221 , train loss: 19.621 , validation loss: 19.584\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 222 , train loss: 19.469 , validation loss: 20.089\n","Epoch: 223 , train loss: 19.519 , validation loss: 20.733\n","Epoch: 224 , train loss: 19.441 , validation loss: 19.701\n","Epoch: 225 , train loss: 19.251 , validation loss: 20.688\n","Epoch: 226 , train loss: 19.235 , validation loss: 20.199\n","Epoch: 227 , train loss: 19.120 , validation loss: 19.328\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 228 , train loss: 19.146 , validation loss: 19.692\n","Epoch: 229 , train loss: 19.020 , validation loss: 20.994\n","Epoch: 230 , train loss: 18.937 , validation loss: 19.680\n","Epoch: 231 , train loss: 18.919 , validation loss: 19.826\n","Epoch: 232 , train loss: 18.836 , validation loss: 19.414\n","Epoch: 233 , train loss: 18.871 , validation loss: 19.517\n","Epoch: 234 , train loss: 18.821 , validation loss: 18.745\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 235 , train loss: 18.660 , validation loss: 19.868\n","Epoch: 236 , train loss: 18.553 , validation loss: 18.380\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 237 , train loss: 18.465 , validation loss: 19.291\n","Epoch: 238 , train loss: 18.516 , validation loss: 19.622\n","Epoch: 239 , train loss: 18.510 , validation loss: 18.952\n","Epoch: 240 , train loss: 18.402 , validation loss: 18.245\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 241 , train loss: 18.274 , validation loss: 18.751\n","Epoch: 242 , train loss: 18.274 , validation loss: 18.771\n","Epoch: 243 , train loss: 18.205 , validation loss: 18.549\n","Epoch: 244 , train loss: 18.132 , validation loss: 18.343\n","Epoch: 245 , train loss: 17.977 , validation loss: 18.827\n","Epoch: 246 , train loss: 17.959 , validation loss: 18.128\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 247 , train loss: 17.862 , validation loss: 18.805\n","Epoch: 248 , train loss: 17.820 , validation loss: 18.438\n","Epoch: 249 , train loss: 17.791 , validation loss: 18.671\n","Epoch: 250 , train loss: 17.474 , validation loss: 18.501\n","Epoch: 251 , train loss: 17.679 , validation loss: 17.572\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 252 , train loss: 17.609 , validation loss: 18.580\n","Epoch: 253 , train loss: 17.405 , validation loss: 17.966\n","Epoch: 254 , train loss: 17.401 , validation loss: 17.714\n","Epoch: 255 , train loss: 17.405 , validation loss: 18.533\n","Epoch: 256 , train loss: 17.284 , validation loss: 18.097\n","Epoch: 257 , train loss: 17.345 , validation loss: 17.661\n","Epoch: 258 , train loss: 17.227 , validation loss: 18.823\n","Epoch: 259 , train loss: 17.002 , validation loss: 17.691\n","Epoch: 260 , train loss: 17.079 , validation loss: 17.644\n","Epoch: 261 , train loss: 17.044 , validation loss: 18.216\n","Epoch: 262 , train loss: 16.976 , validation loss: 17.488\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 263 , train loss: 16.877 , validation loss: 16.550\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 264 , train loss: 16.753 , validation loss: 17.337\n","Epoch: 265 , train loss: 16.637 , validation loss: 16.591\n","Epoch: 266 , train loss: 16.675 , validation loss: 17.415\n","Epoch: 267 , train loss: 16.510 , validation loss: 16.925\n","Epoch: 268 , train loss: 16.641 , validation loss: 17.234\n","Epoch: 269 , train loss: 16.382 , validation loss: 17.149\n","Epoch: 270 , train loss: 16.441 , validation loss: 17.308\n","Epoch: 271 , train loss: 16.365 , validation loss: 17.055\n","Epoch: 272 , train loss: 16.423 , validation loss: 16.895\n","Epoch: 273 , train loss: 16.409 , validation loss: 16.953\n","Epoch: 274 , train loss: 16.238 , validation loss: 16.108\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 275 , train loss: 16.184 , validation loss: 16.398\n","Epoch: 276 , train loss: 16.129 , validation loss: 17.319\n","Epoch: 277 , train loss: 16.030 , validation loss: 16.884\n","Epoch: 278 , train loss: 15.918 , validation loss: 16.358\n","Epoch: 279 , train loss: 16.015 , validation loss: 16.197\n","Epoch: 280 , train loss: 15.877 , validation loss: 15.898\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 281 , train loss: 15.871 , validation loss: 15.317\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 282 , train loss: 15.691 , validation loss: 16.780\n","Epoch: 283 , train loss: 15.679 , validation loss: 15.738\n","Epoch: 284 , train loss: 15.590 , validation loss: 15.924\n","Epoch: 285 , train loss: 15.574 , validation loss: 15.666\n","Epoch: 286 , train loss: 15.388 , validation loss: 15.569\n","Epoch: 287 , train loss: 15.265 , validation loss: 15.810\n","Epoch: 288 , train loss: 15.412 , validation loss: 16.143\n","Epoch: 289 , train loss: 15.251 , validation loss: 15.403\n","Epoch: 290 , train loss: 15.261 , validation loss: 15.488\n","Epoch: 291 , train loss: 15.236 , validation loss: 15.928\n","Epoch: 292 , train loss: 15.050 , validation loss: 15.894\n","Epoch: 293 , train loss: 15.046 , validation loss: 15.454\n","Epoch: 294 , train loss: 14.869 , validation loss: 15.573\n","Epoch: 295 , train loss: 14.877 , validation loss: 15.131\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 296 , train loss: 14.924 , validation loss: 15.332\n","Epoch: 297 , train loss: 14.768 , validation loss: 15.137\n","Epoch: 298 , train loss: 14.711 , validation loss: 15.881\n","Epoch: 299 , train loss: 14.649 , validation loss: 14.915\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 300 , train loss: 14.526 , validation loss: 14.025\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 301 , train loss: 14.617 , validation loss: 14.935\n","Epoch: 302 , train loss: 14.351 , validation loss: 14.555\n","Epoch: 303 , train loss: 14.444 , validation loss: 14.117\n","Epoch: 304 , train loss: 14.382 , validation loss: 14.829\n","Epoch: 305 , train loss: 14.158 , validation loss: 14.756\n","Epoch: 306 , train loss: 14.161 , validation loss: 14.774\n","Epoch: 307 , train loss: 14.194 , validation loss: 14.756\n","Epoch: 308 , train loss: 14.030 , validation loss: 14.116\n","Epoch: 309 , train loss: 14.115 , validation loss: 14.360\n","Epoch: 310 , train loss: 14.061 , validation loss: 14.384\n","Epoch: 311 , train loss: 14.016 , validation loss: 14.392\n","Epoch: 312 , train loss: 14.001 , validation loss: 13.762\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 313 , train loss: 13.956 , validation loss: 13.634\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 314 , train loss: 13.816 , validation loss: 13.912\n","Epoch: 315 , train loss: 13.728 , validation loss: 14.025\n","Epoch: 316 , train loss: 13.635 , validation loss: 13.744\n","Epoch: 317 , train loss: 13.537 , validation loss: 13.901\n","Epoch: 318 , train loss: 13.667 , validation loss: 13.833\n","Epoch: 319 , train loss: 13.633 , validation loss: 13.155\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 320 , train loss: 13.507 , validation loss: 13.708\n","Epoch: 321 , train loss: 13.425 , validation loss: 13.187\n","Epoch: 322 , train loss: 13.396 , validation loss: 14.255\n","Epoch: 323 , train loss: 13.419 , validation loss: 12.736\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 324 , train loss: 13.323 , validation loss: 13.795\n","Epoch: 325 , train loss: 13.211 , validation loss: 13.587\n","Epoch: 326 , train loss: 13.051 , validation loss: 13.277\n","Epoch: 327 , train loss: 13.072 , validation loss: 12.744\n","Epoch: 328 , train loss: 12.976 , validation loss: 13.041\n","Epoch: 329 , train loss: 12.995 , validation loss: 13.467\n","Epoch: 330 , train loss: 12.840 , validation loss: 12.977\n","Epoch: 331 , train loss: 12.892 , validation loss: 12.919\n","Epoch: 332 , train loss: 12.806 , validation loss: 12.325\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 333 , train loss: 12.824 , validation loss: 13.914\n","Epoch: 334 , train loss: 12.688 , validation loss: 12.603\n","Epoch: 335 , train loss: 12.744 , validation loss: 12.439\n","Epoch: 336 , train loss: 12.510 , validation loss: 13.020\n","Epoch: 337 , train loss: 12.539 , validation loss: 12.780\n","Epoch: 338 , train loss: 12.513 , validation loss: 12.568\n","Epoch: 339 , train loss: 12.301 , validation loss: 12.686\n","Epoch: 340 , train loss: 12.352 , validation loss: 12.705\n","Epoch: 341 , train loss: 12.384 , validation loss: 11.699\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 342 , train loss: 12.419 , validation loss: 13.850\n","Epoch: 343 , train loss: 12.164 , validation loss: 12.321\n","Epoch: 344 , train loss: 12.132 , validation loss: 12.141\n","Epoch: 345 , train loss: 12.046 , validation loss: 12.316\n","Epoch: 346 , train loss: 11.836 , validation loss: 11.535\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 347 , train loss: 11.915 , validation loss: 12.978\n","Epoch: 348 , train loss: 11.846 , validation loss: 11.539\n","Epoch: 349 , train loss: 11.571 , validation loss: 12.655\n","Epoch: 350 , train loss: 11.707 , validation loss: 11.753\n","Epoch: 351 , train loss: 11.709 , validation loss: 12.943\n","Epoch: 352 , train loss: 11.617 , validation loss: 12.464\n","Epoch: 353 , train loss: 11.570 , validation loss: 11.570\n","Epoch: 354 , train loss: 11.565 , validation loss: 11.586\n","Epoch: 355 , train loss: 11.403 , validation loss: 12.139\n","Epoch: 356 , train loss: 11.416 , validation loss: 11.508\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 357 , train loss: 11.368 , validation loss: 11.215\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 358 , train loss: 11.341 , validation loss: 12.036\n","Epoch: 359 , train loss: 11.195 , validation loss: 10.714\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 360 , train loss: 11.318 , validation loss: 11.357\n","Epoch: 361 , train loss: 11.161 , validation loss: 11.059\n","Epoch: 362 , train loss: 11.062 , validation loss: 11.378\n","Epoch: 363 , train loss: 10.916 , validation loss: 12.078\n","Epoch: 364 , train loss: 11.017 , validation loss: 10.974\n","Epoch: 365 , train loss: 11.002 , validation loss: 11.054\n","Epoch: 366 , train loss: 10.883 , validation loss: 11.089\n","Epoch: 367 , train loss: 10.955 , validation loss: 11.287\n","Epoch: 368 , train loss: 10.770 , validation loss: 10.748\n","Epoch: 369 , train loss: 10.757 , validation loss: 11.321\n","Epoch: 370 , train loss: 10.863 , validation loss: 11.111\n","Epoch: 371 , train loss: 10.703 , validation loss: 10.824\n","Epoch: 372 , train loss: 10.581 , validation loss: 10.723\n","Epoch: 373 , train loss: 10.604 , validation loss: 10.369\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 374 , train loss: 10.651 , validation loss: 10.879\n","Epoch: 375 , train loss: 10.479 , validation loss: 11.109\n","Epoch: 376 , train loss: 10.518 , validation loss: 10.163\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 377 , train loss: 10.309 , validation loss: 10.686\n","Epoch: 378 , train loss: 10.280 , validation loss: 10.263\n","Epoch: 379 , train loss: 10.405 , validation loss: 10.870\n","Epoch: 380 , train loss: 10.173 , validation loss: 10.358\n","Epoch: 381 , train loss: 10.329 , validation loss: 10.695\n","Epoch: 382 , train loss: 10.106 , validation loss: 9.883\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 383 , train loss: 10.085 , validation loss: 10.131\n","Epoch: 384 , train loss: 10.047 , validation loss: 10.146\n","Epoch: 385 , train loss: 10.101 , validation loss: 10.011\n","Epoch: 386 , train loss: 10.067 , validation loss: 9.934\n","Epoch: 387 , train loss: 10.058 , validation loss: 9.725\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 388 , train loss: 10.064 , validation loss: 9.780\n","Epoch: 389 , train loss: 9.909 , validation loss: 10.426\n","Epoch: 390 , train loss: 9.753 , validation loss: 10.137\n","Epoch: 391 , train loss: 9.787 , validation loss: 10.181\n","Epoch: 392 , train loss: 9.849 , validation loss: 9.665\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 393 , train loss: 9.787 , validation loss: 9.564\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 394 , train loss: 9.706 , validation loss: 9.634\n","Epoch: 395 , train loss: 9.604 , validation loss: 10.130\n","Epoch: 396 , train loss: 9.653 , validation loss: 9.430\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 397 , train loss: 9.679 , validation loss: 9.307\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 398 , train loss: 9.586 , validation loss: 9.604\n","Epoch: 399 , train loss: 9.473 , validation loss: 9.611\n","Epoch: 400 , train loss: 9.478 , validation loss: 9.365\n","Epoch: 401 , train loss: 9.542 , validation loss: 9.561\n","Epoch: 402 , train loss: 9.342 , validation loss: 9.288\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 403 , train loss: 9.302 , validation loss: 8.789\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 404 , train loss: 9.350 , validation loss: 9.742\n","Epoch: 405 , train loss: 9.285 , validation loss: 9.042\n","Epoch: 406 , train loss: 9.324 , validation loss: 9.025\n","Epoch: 407 , train loss: 9.267 , validation loss: 9.368\n","Epoch: 408 , train loss: 9.259 , validation loss: 8.902\n","Epoch: 409 , train loss: 9.270 , validation loss: 8.980\n","Epoch: 410 , train loss: 9.081 , validation loss: 9.180\n","Epoch: 411 , train loss: 9.191 , validation loss: 9.169\n","Epoch: 412 , train loss: 9.171 , validation loss: 8.967\n","Epoch: 413 , train loss: 9.174 , validation loss: 9.286\n","Epoch: 414 , train loss: 9.031 , validation loss: 8.757\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 415 , train loss: 9.023 , validation loss: 9.024\n","Epoch: 416 , train loss: 9.092 , validation loss: 8.790\n","Epoch: 417 , train loss: 9.044 , validation loss: 8.848\n","Epoch: 418 , train loss: 9.007 , validation loss: 9.396\n","Epoch: 419 , train loss: 8.901 , validation loss: 8.825\n","Epoch: 420 , train loss: 8.853 , validation loss: 8.485\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 421 , train loss: 8.982 , validation loss: 8.820\n","Epoch: 422 , train loss: 8.934 , validation loss: 8.659\n","Epoch: 423 , train loss: 8.875 , validation loss: 8.579\n","Epoch: 424 , train loss: 8.963 , validation loss: 8.231\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 425 , train loss: 8.815 , validation loss: 8.284\n","Epoch: 426 , train loss: 8.834 , validation loss: 9.179\n","Epoch: 427 , train loss: 8.799 , validation loss: 8.471\n","Epoch: 428 , train loss: 8.859 , validation loss: 8.683\n","Epoch: 429 , train loss: 8.727 , validation loss: 8.663\n","Epoch: 430 , train loss: 8.615 , validation loss: 8.933\n","Epoch: 431 , train loss: 8.640 , validation loss: 8.158\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 432 , train loss: 8.641 , validation loss: 9.094\n","Epoch: 433 , train loss: 8.573 , validation loss: 8.374\n","Epoch: 434 , train loss: 8.713 , validation loss: 8.505\n","Epoch: 435 , train loss: 8.520 , validation loss: 8.168\n","Epoch: 436 , train loss: 8.463 , validation loss: 8.577\n","Epoch: 437 , train loss: 8.537 , validation loss: 8.513\n","Epoch: 438 , train loss: 8.489 , validation loss: 8.356\n","Epoch: 439 , train loss: 8.421 , validation loss: 8.005\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 440 , train loss: 8.359 , validation loss: 8.529\n","Epoch: 441 , train loss: 8.225 , validation loss: 8.098\n","Epoch: 442 , train loss: 8.317 , validation loss: 7.970\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 443 , train loss: 8.077 , validation loss: 7.997\n","Epoch: 444 , train loss: 8.257 , validation loss: 8.679\n","Epoch: 445 , train loss: 8.171 , validation loss: 8.620\n","Epoch: 446 , train loss: 8.252 , validation loss: 7.800\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 447 , train loss: 8.080 , validation loss: 7.806\n","Epoch: 448 , train loss: 8.090 , validation loss: 8.566\n","Epoch: 449 , train loss: 8.298 , validation loss: 8.106\n","Epoch: 450 , train loss: 8.078 , validation loss: 8.123\n","Epoch: 451 , train loss: 7.986 , validation loss: 8.228\n","Epoch: 452 , train loss: 7.980 , validation loss: 7.726\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 453 , train loss: 7.966 , validation loss: 7.848\n","Epoch: 454 , train loss: 7.940 , validation loss: 7.664\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 455 , train loss: 8.031 , validation loss: 7.952\n","Epoch: 456 , train loss: 8.019 , validation loss: 7.863\n","Epoch: 457 , train loss: 8.042 , validation loss: 7.670\n","Epoch: 458 , train loss: 7.940 , validation loss: 7.620\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 459 , train loss: 7.940 , validation loss: 7.851\n","Epoch: 460 , train loss: 7.885 , validation loss: 7.854\n","Epoch: 461 , train loss: 7.848 , validation loss: 7.679\n","Epoch: 462 , train loss: 7.901 , validation loss: 7.921\n","Epoch: 463 , train loss: 7.946 , validation loss: 7.557\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 464 , train loss: 7.985 , validation loss: 7.859\n","Epoch: 465 , train loss: 7.821 , validation loss: 7.627\n","Epoch: 466 , train loss: 7.686 , validation loss: 7.610\n","Epoch: 467 , train loss: 7.838 , validation loss: 7.538\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 468 , train loss: 7.817 , validation loss: 7.543\n","Epoch: 469 , train loss: 7.909 , validation loss: 7.683\n","Epoch: 470 , train loss: 7.758 , validation loss: 7.560\n","Epoch: 471 , train loss: 7.735 , validation loss: 7.463\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 472 , train loss: 7.884 , validation loss: 7.403\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 473 , train loss: 7.865 , validation loss: 7.364\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 474 , train loss: 7.763 , validation loss: 8.005\n","Epoch: 475 , train loss: 7.720 , validation loss: 7.504\n","Epoch: 476 , train loss: 7.728 , validation loss: 7.368\n","Epoch: 477 , train loss: 7.645 , validation loss: 7.375\n","Epoch: 478 , train loss: 7.671 , validation loss: 7.404\n","Epoch: 479 , train loss: 7.848 , validation loss: 7.340\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 480 , train loss: 7.621 , validation loss: 7.243\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 481 , train loss: 7.415 , validation loss: 7.299\n","Epoch: 482 , train loss: 7.833 , validation loss: 7.308\n","Epoch: 483 , train loss: 7.658 , validation loss: 7.313\n","Epoch: 484 , train loss: 7.580 , validation loss: 7.430\n","Epoch: 485 , train loss: 7.601 , validation loss: 7.211\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 486 , train loss: 7.706 , validation loss: 7.708\n","Epoch: 487 , train loss: 7.658 , validation loss: 7.444\n","Epoch: 488 , train loss: 7.609 , validation loss: 7.342\n","Epoch: 489 , train loss: 7.560 , validation loss: 7.275\n","Epoch: 490 , train loss: 7.629 , validation loss: 7.478\n","Epoch: 491 , train loss: 7.566 , validation loss: 7.257\n","Epoch: 492 , train loss: 7.670 , validation loss: 7.636\n","Epoch: 493 , train loss: 7.604 , validation loss: 7.253\n","Epoch: 494 , train loss: 7.569 , validation loss: 7.270\n","Epoch: 495 , train loss: 7.581 , validation loss: 7.337\n","Epoch: 496 , train loss: 7.519 , validation loss: 7.240\n","Epoch: 497 , train loss: 7.532 , validation loss: 7.241\n","Epoch: 498 , train loss: 7.696 , validation loss: 7.231\n","Epoch: 499 , train loss: 7.668 , validation loss: 7.309\n","Epoch: 500 , train loss: 7.480 , validation loss: 7.225\n","Epoch: 501 , train loss: 7.501 , validation loss: 7.176\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 502 , train loss: 7.490 , validation loss: 7.234\n","Epoch: 503 , train loss: 7.456 , validation loss: 7.368\n","Epoch: 504 , train loss: 7.516 , validation loss: 7.216\n","Epoch: 505 , train loss: 7.534 , validation loss: 7.159\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 506 , train loss: 7.559 , validation loss: 7.229\n","Epoch: 507 , train loss: 7.439 , validation loss: 7.263\n","Epoch: 508 , train loss: 7.454 , validation loss: 7.111\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 509 , train loss: 7.451 , validation loss: 7.241\n","Epoch: 510 , train loss: 7.370 , validation loss: 7.203\n","Epoch: 511 , train loss: 7.507 , validation loss: 7.121\n","Epoch: 512 , train loss: 7.450 , validation loss: 7.172\n","Epoch: 513 , train loss: 7.366 , validation loss: 7.030\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 514 , train loss: 7.437 , validation loss: 7.115\n","Epoch: 515 , train loss: 7.352 , validation loss: 7.039\n","Epoch: 516 , train loss: 7.554 , validation loss: 7.063\n","Epoch: 517 , train loss: 7.425 , validation loss: 7.415\n","Epoch: 518 , train loss: 7.381 , validation loss: 7.175\n","Epoch: 519 , train loss: 7.375 , validation loss: 7.089\n","Epoch: 520 , train loss: 7.388 , validation loss: 7.058\n","Epoch: 521 , train loss: 7.422 , validation loss: 7.016\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 522 , train loss: 7.500 , validation loss: 6.988\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 523 , train loss: 7.423 , validation loss: 6.984\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 524 , train loss: 7.504 , validation loss: 7.041\n","Epoch: 525 , train loss: 7.420 , validation loss: 7.112\n","Epoch: 526 , train loss: 7.470 , validation loss: 7.098\n","Epoch: 527 , train loss: 7.464 , validation loss: 7.340\n","Epoch: 528 , train loss: 7.370 , validation loss: 7.103\n","Epoch: 529 , train loss: 7.347 , validation loss: 7.028\n","Epoch: 530 , train loss: 7.391 , validation loss: 7.310\n","Epoch: 531 , train loss: 7.486 , validation loss: 7.230\n","Epoch: 532 , train loss: 7.321 , validation loss: 7.145\n","Epoch: 533 , train loss: 7.432 , validation loss: 7.264\n","Epoch: 534 , train loss: 7.405 , validation loss: 7.172\n","Epoch: 535 , train loss: 7.262 , validation loss: 7.195\n","Epoch: 536 , train loss: 7.330 , validation loss: 7.061\n","Epoch: 537 , train loss: 7.402 , validation loss: 7.175\n","Epoch: 538 , train loss: 7.253 , validation loss: 7.211\n","Epoch: 539 , train loss: 7.457 , validation loss: 7.072\n","Epoch: 540 , train loss: 7.370 , validation loss: 7.163\n","Epoch: 541 , train loss: 7.325 , validation loss: 6.974\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 542 , train loss: 7.346 , validation loss: 7.206\n","Epoch: 543 , train loss: 7.281 , validation loss: 7.111\n","Epoch: 544 , train loss: 7.440 , validation loss: 7.145\n","Epoch: 545 , train loss: 7.408 , validation loss: 7.037\n","Epoch: 546 , train loss: 7.314 , validation loss: 7.052\n","Epoch: 547 , train loss: 7.249 , validation loss: 6.989\n","Epoch: 548 , train loss: 7.420 , validation loss: 7.098\n","Epoch: 549 , train loss: 7.345 , validation loss: 7.029\n","Epoch: 550 , train loss: 7.235 , validation loss: 7.019\n","Epoch: 551 , train loss: 7.413 , validation loss: 7.106\n","Epoch: 552 , train loss: 7.331 , validation loss: 7.001\n","Epoch: 553 , train loss: 7.351 , validation loss: 7.059\n","Epoch: 554 , train loss: 7.265 , validation loss: 7.313\n","Epoch: 555 , train loss: 7.287 , validation loss: 6.908\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 556 , train loss: 7.199 , validation loss: 7.094\n","Epoch: 557 , train loss: 7.251 , validation loss: 7.061\n","Epoch: 558 , train loss: 7.210 , validation loss: 7.110\n","Epoch: 559 , train loss: 7.221 , validation loss: 7.051\n","Epoch: 560 , train loss: 7.453 , validation loss: 6.943\n","Epoch: 561 , train loss: 7.338 , validation loss: 7.105\n","Epoch: 562 , train loss: 7.371 , validation loss: 7.000\n","Epoch: 563 , train loss: 7.202 , validation loss: 7.057\n","Epoch: 564 , train loss: 7.192 , validation loss: 7.053\n","Epoch: 565 , train loss: 7.075 , validation loss: 6.857\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 566 , train loss: 7.379 , validation loss: 6.899\n","Epoch: 567 , train loss: 7.340 , validation loss: 7.032\n","Epoch: 568 , train loss: 7.296 , validation loss: 6.900\n","Epoch: 569 , train loss: 7.182 , validation loss: 6.997\n","Epoch: 570 , train loss: 7.306 , validation loss: 7.067\n","Epoch: 571 , train loss: 7.148 , validation loss: 6.962\n","Epoch: 572 , train loss: 7.270 , validation loss: 6.952\n","Epoch: 573 , train loss: 7.287 , validation loss: 6.932\n","Epoch: 574 , train loss: 7.308 , validation loss: 6.847\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 575 , train loss: 7.146 , validation loss: 6.981\n","Epoch: 576 , train loss: 7.232 , validation loss: 7.012\n","Epoch: 577 , train loss: 7.189 , validation loss: 6.852\n","Epoch: 578 , train loss: 7.244 , validation loss: 6.895\n","Epoch: 579 , train loss: 7.081 , validation loss: 6.892\n","Epoch: 580 , train loss: 7.072 , validation loss: 7.038\n","Epoch: 581 , train loss: 7.072 , validation loss: 6.756\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 582 , train loss: 7.099 , validation loss: 6.789\n","Epoch: 583 , train loss: 7.093 , validation loss: 6.837\n","Epoch: 584 , train loss: 7.102 , validation loss: 6.818\n","Epoch: 585 , train loss: 7.185 , validation loss: 6.853\n","Epoch: 586 , train loss: 7.176 , validation loss: 6.952\n","Epoch: 587 , train loss: 7.186 , validation loss: 6.845\n","Epoch: 588 , train loss: 7.136 , validation loss: 6.765\n","Epoch: 589 , train loss: 7.158 , validation loss: 6.914\n","Epoch: 590 , train loss: 7.076 , validation loss: 6.835\n","Epoch: 591 , train loss: 7.107 , validation loss: 6.804\n","Epoch: 592 , train loss: 7.035 , validation loss: 6.914\n","Epoch: 593 , train loss: 7.167 , validation loss: 6.912\n","Epoch: 594 , train loss: 7.116 , validation loss: 6.831\n","Epoch: 595 , train loss: 7.156 , validation loss: 6.899\n","Epoch: 596 , train loss: 7.204 , validation loss: 6.922\n","Epoch: 597 , train loss: 7.137 , validation loss: 6.912\n","Epoch: 598 , train loss: 7.051 , validation loss: 6.848\n","Epoch: 599 , train loss: 7.214 , validation loss: 6.844\n","Epoch: 600 , train loss: 7.218 , validation loss: 6.997\n","Epoch: 601 , train loss: 7.269 , validation loss: 6.983\n","Epoch: 602 , train loss: 7.050 , validation loss: 6.950\n","Epoch: 603 , train loss: 7.068 , validation loss: 6.876\n","Epoch: 604 , train loss: 6.972 , validation loss: 6.892\n","Epoch: 605 , train loss: 7.015 , validation loss: 6.859\n","Epoch: 606 , train loss: 7.160 , validation loss: 6.908\n","Epoch: 607 , train loss: 7.222 , validation loss: 6.913\n","Epoch: 608 , train loss: 7.095 , validation loss: 6.881\n","Epoch: 609 , train loss: 6.976 , validation loss: 6.926\n","Epoch: 610 , train loss: 7.161 , validation loss: 6.901\n","Epoch: 611 , train loss: 7.189 , validation loss: 6.869\n","Epoch: 612 , train loss: 7.106 , validation loss: 6.972\n","Epoch: 613 , train loss: 7.250 , validation loss: 6.846\n","Epoch: 614 , train loss: 7.062 , validation loss: 6.845\n","Epoch: 615 , train loss: 7.080 , validation loss: 6.937\n","Epoch: 616 , train loss: 7.052 , validation loss: 6.848\n","Epoch: 617 , train loss: 7.067 , validation loss: 6.889\n","Epoch: 618 , train loss: 7.069 , validation loss: 6.994\n","Epoch: 619 , train loss: 7.073 , validation loss: 6.981\n","Epoch: 620 , train loss: 7.042 , validation loss: 6.854\n","Epoch: 621 , train loss: 7.160 , validation loss: 6.889\n","Epoch: 622 , train loss: 7.078 , validation loss: 6.956\n","Epoch: 623 , train loss: 7.045 , validation loss: 7.068\n","Epoch: 624 , train loss: 7.063 , validation loss: 6.926\n","Epoch: 625 , train loss: 7.188 , validation loss: 6.891\n","Epoch: 626 , train loss: 7.121 , validation loss: 7.021\n","Epoch: 627 , train loss: 7.114 , validation loss: 6.952\n","Epoch: 628 , train loss: 7.140 , validation loss: 7.020\n","Epoch: 629 , train loss: 7.104 , validation loss: 6.864\n","Epoch: 630 , train loss: 7.114 , validation loss: 6.841\n","Epoch: 631 , train loss: 7.000 , validation loss: 6.902\n","Epoch: 632 , train loss: 7.084 , validation loss: 6.896\n","Epoch: 633 , train loss: 7.125 , validation loss: 7.001\n","Epoch: 634 , train loss: 7.214 , validation loss: 6.841\n","Epoch: 635 , train loss: 7.127 , validation loss: 6.938\n","Epoch: 636 , train loss: 7.140 , validation loss: 6.924\n","Epoch: 637 , train loss: 7.058 , validation loss: 6.847\n","Epoch: 638 , train loss: 6.979 , validation loss: 6.880\n","Epoch: 639 , train loss: 7.108 , validation loss: 6.855\n","Epoch: 640 , train loss: 7.090 , validation loss: 6.983\n","Epoch: 641 , train loss: 7.032 , validation loss: 6.805\n","Epoch: 642 , train loss: 7.042 , validation loss: 6.896\n","Epoch: 643 , train loss: 7.087 , validation loss: 6.905\n","Epoch: 644 , train loss: 6.936 , validation loss: 6.825\n","Epoch: 645 , train loss: 7.166 , validation loss: 6.880\n","Epoch: 646 , train loss: 6.959 , validation loss: 6.841\n","Epoch: 647 , train loss: 6.943 , validation loss: 6.946\n","Epoch: 648 , train loss: 7.009 , validation loss: 6.900\n","Epoch: 649 , train loss: 7.002 , validation loss: 6.981\n","Epoch: 650 , train loss: 7.004 , validation loss: 6.816\n","Epoch: 651 , train loss: 7.026 , validation loss: 6.882\n","Epoch: 652 , train loss: 7.080 , validation loss: 6.894\n","Epoch: 653 , train loss: 7.032 , validation loss: 6.841\n","Epoch: 654 , train loss: 7.146 , validation loss: 6.927\n","Epoch: 655 , train loss: 6.980 , validation loss: 6.844\n","Epoch: 656 , train loss: 7.100 , validation loss: 6.790\n","Epoch: 657 , train loss: 7.117 , validation loss: 6.839\n","Epoch: 658 , train loss: 6.947 , validation loss: 6.832\n","Epoch: 659 , train loss: 6.971 , validation loss: 6.945\n","Epoch: 660 , train loss: 7.030 , validation loss: 6.858\n","Epoch: 661 , train loss: 7.003 , validation loss: 6.946\n","Epoch: 662 , train loss: 7.010 , validation loss: 6.825\n","Epoch: 663 , train loss: 6.954 , validation loss: 6.807\n","Epoch: 664 , train loss: 6.938 , validation loss: 6.903\n","Epoch: 665 , train loss: 7.073 , validation loss: 6.840\n","Epoch: 666 , train loss: 6.990 , validation loss: 6.898\n","Epoch: 667 , train loss: 7.059 , validation loss: 6.859\n","Epoch: 668 , train loss: 6.964 , validation loss: 6.850\n","Epoch: 669 , train loss: 6.949 , validation loss: 6.836\n","Epoch: 670 , train loss: 7.016 , validation loss: 7.006\n","Epoch: 671 , train loss: 6.950 , validation loss: 6.698\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 672 , train loss: 7.114 , validation loss: 6.849\n","Epoch: 673 , train loss: 7.033 , validation loss: 6.954\n","Epoch: 674 , train loss: 7.092 , validation loss: 6.855\n","Epoch: 675 , train loss: 6.971 , validation loss: 6.773\n","Epoch: 676 , train loss: 6.965 , validation loss: 6.769\n","Epoch: 677 , train loss: 7.077 , validation loss: 6.810\n","Epoch: 678 , train loss: 7.016 , validation loss: 6.773\n","Epoch: 679 , train loss: 6.840 , validation loss: 6.873\n","Epoch: 680 , train loss: 6.995 , validation loss: 6.814\n","Epoch: 681 , train loss: 6.775 , validation loss: 6.762\n","Epoch: 682 , train loss: 6.993 , validation loss: 6.778\n","Epoch: 683 , train loss: 6.962 , validation loss: 6.791\n","Epoch: 684 , train loss: 7.039 , validation loss: 6.917\n","Epoch: 685 , train loss: 7.096 , validation loss: 6.903\n","Epoch: 686 , train loss: 7.082 , validation loss: 6.788\n","Epoch: 687 , train loss: 6.836 , validation loss: 6.826\n","Epoch: 688 , train loss: 6.951 , validation loss: 6.883\n","Epoch: 689 , train loss: 6.915 , validation loss: 6.829\n","Epoch: 690 , train loss: 6.896 , validation loss: 6.898\n","Epoch: 691 , train loss: 6.963 , validation loss: 6.814\n","Epoch: 692 , train loss: 6.905 , validation loss: 6.810\n","Epoch: 693 , train loss: 7.109 , validation loss: 6.916\n","Epoch: 694 , train loss: 6.869 , validation loss: 6.818\n","Epoch: 695 , train loss: 7.000 , validation loss: 6.818\n","Epoch: 696 , train loss: 6.995 , validation loss: 6.790\n","Epoch: 697 , train loss: 6.846 , validation loss: 6.844\n","Epoch: 698 , train loss: 6.731 , validation loss: 6.790\n","Epoch: 699 , train loss: 6.995 , validation loss: 6.759\n","Epoch: 700 , train loss: 6.957 , validation loss: 6.888\n","Epoch: 701 , train loss: 6.878 , validation loss: 6.883\n","Epoch: 702 , train loss: 6.804 , validation loss: 6.791\n","Epoch: 703 , train loss: 6.936 , validation loss: 6.787\n","Epoch: 704 , train loss: 6.903 , validation loss: 6.788\n","Epoch: 705 , train loss: 6.919 , validation loss: 6.870\n","Epoch: 706 , train loss: 6.951 , validation loss: 6.800\n","Epoch: 707 , train loss: 6.860 , validation loss: 6.800\n","Epoch: 708 , train loss: 6.971 , validation loss: 6.867\n","Epoch: 709 , train loss: 6.870 , validation loss: 6.783\n","Epoch: 710 , train loss: 6.874 , validation loss: 6.826\n","Epoch: 711 , train loss: 6.898 , validation loss: 6.794\n","Epoch: 712 , train loss: 7.018 , validation loss: 6.772\n","Epoch: 713 , train loss: 6.933 , validation loss: 6.938\n","Epoch: 714 , train loss: 6.834 , validation loss: 6.880\n","Epoch: 715 , train loss: 6.777 , validation loss: 6.778\n","Epoch: 716 , train loss: 6.844 , validation loss: 6.764\n","Epoch: 717 , train loss: 6.926 , validation loss: 6.839\n","Epoch: 718 , train loss: 6.825 , validation loss: 6.778\n","Epoch: 719 , train loss: 6.979 , validation loss: 6.811\n","Epoch: 720 , train loss: 6.664 , validation loss: 6.830\n","Epoch: 721 , train loss: 6.882 , validation loss: 6.790\n","Epoch: 722 , train loss: 6.891 , validation loss: 6.646\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 723 , train loss: 6.809 , validation loss: 6.829\n","Epoch: 724 , train loss: 6.843 , validation loss: 6.731\n","Epoch: 725 , train loss: 6.923 , validation loss: 6.721\n","Epoch: 726 , train loss: 6.891 , validation loss: 6.706\n","Epoch: 727 , train loss: 6.756 , validation loss: 6.821\n","Epoch: 728 , train loss: 6.799 , validation loss: 6.846\n","Epoch: 729 , train loss: 6.833 , validation loss: 6.697\n","Epoch: 730 , train loss: 6.700 , validation loss: 6.799\n","Epoch: 731 , train loss: 6.746 , validation loss: 6.708\n","Epoch: 732 , train loss: 6.843 , validation loss: 6.755\n","Epoch: 733 , train loss: 6.801 , validation loss: 6.749\n","Epoch: 734 , train loss: 6.817 , validation loss: 6.795\n","Epoch: 735 , train loss: 6.802 , validation loss: 6.777\n","Epoch: 736 , train loss: 6.936 , validation loss: 6.805\n","Epoch: 737 , train loss: 6.831 , validation loss: 6.782\n","Epoch: 738 , train loss: 6.731 , validation loss: 6.832\n","Epoch: 739 , train loss: 6.891 , validation loss: 6.776\n","Epoch: 740 , train loss: 6.859 , validation loss: 6.731\n","Epoch: 741 , train loss: 6.799 , validation loss: 6.767\n","Epoch: 742 , train loss: 6.934 , validation loss: 6.861\n","Epoch: 743 , train loss: 6.852 , validation loss: 6.859\n","Epoch: 744 , train loss: 6.951 , validation loss: 6.832\n","Epoch: 745 , train loss: 6.913 , validation loss: 6.719\n","Epoch: 746 , train loss: 6.831 , validation loss: 6.830\n","Epoch: 747 , train loss: 6.820 , validation loss: 6.815\n","Epoch: 748 , train loss: 6.684 , validation loss: 6.730\n","Epoch: 749 , train loss: 6.976 , validation loss: 6.765\n","Epoch: 750 , train loss: 6.844 , validation loss: 6.809\n","Epoch: 751 , train loss: 6.691 , validation loss: 6.789\n","Epoch: 752 , train loss: 6.785 , validation loss: 6.743\n","Epoch: 753 , train loss: 6.875 , validation loss: 6.821\n","Epoch: 754 , train loss: 6.780 , validation loss: 6.743\n","Epoch: 755 , train loss: 6.763 , validation loss: 6.718\n","Epoch: 756 , train loss: 6.812 , validation loss: 6.695\n","Epoch: 757 , train loss: 6.934 , validation loss: 6.872\n","Epoch: 758 , train loss: 6.953 , validation loss: 6.701\n","Epoch: 759 , train loss: 6.966 , validation loss: 6.682\n","Epoch: 760 , train loss: 6.651 , validation loss: 6.722\n","Epoch: 761 , train loss: 6.887 , validation loss: 6.838\n","Epoch: 762 , train loss: 6.886 , validation loss: 6.749\n","Epoch: 763 , train loss: 6.776 , validation loss: 6.786\n","Epoch: 764 , train loss: 6.965 , validation loss: 6.768\n","Epoch: 765 , train loss: 6.947 , validation loss: 6.752\n","Epoch: 766 , train loss: 6.866 , validation loss: 6.847\n","Epoch: 767 , train loss: 6.831 , validation loss: 6.796\n","Epoch: 768 , train loss: 6.911 , validation loss: 6.825\n","Epoch: 769 , train loss: 6.870 , validation loss: 6.794\n","Epoch: 770 , train loss: 6.626 , validation loss: 6.741\n","Epoch: 771 , train loss: 6.711 , validation loss: 6.703\n","Epoch: 772 , train loss: 6.628 , validation loss: 6.775\n","Epoch: 773 , train loss: 6.838 , validation loss: 6.783\n","Epoch: 774 , train loss: 6.683 , validation loss: 6.781\n","Epoch: 775 , train loss: 6.801 , validation loss: 6.934\n","Epoch: 776 , train loss: 6.850 , validation loss: 6.885\n","Epoch: 777 , train loss: 6.716 , validation loss: 6.727\n","Epoch: 778 , train loss: 6.881 , validation loss: 6.658\n","Epoch: 779 , train loss: 6.822 , validation loss: 6.643\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 780 , train loss: 6.670 , validation loss: 6.681\n","Epoch: 781 , train loss: 6.815 , validation loss: 6.812\n","Epoch: 782 , train loss: 6.953 , validation loss: 6.838\n","Epoch: 783 , train loss: 6.664 , validation loss: 6.680\n","Epoch: 784 , train loss: 6.620 , validation loss: 6.724\n","Epoch: 785 , train loss: 6.836 , validation loss: 6.689\n","Epoch: 786 , train loss: 6.698 , validation loss: 6.739\n","Epoch: 787 , train loss: 6.775 , validation loss: 6.733\n","Epoch: 788 , train loss: 6.770 , validation loss: 6.724\n","Epoch: 789 , train loss: 6.776 , validation loss: 6.746\n","Epoch: 790 , train loss: 6.813 , validation loss: 6.764\n","Epoch: 791 , train loss: 6.784 , validation loss: 6.806\n","Epoch: 792 , train loss: 6.834 , validation loss: 6.762\n","Epoch: 793 , train loss: 6.720 , validation loss: 6.675\n","Epoch: 794 , train loss: 6.879 , validation loss: 6.942\n","Epoch: 795 , train loss: 6.853 , validation loss: 6.660\n","Epoch: 796 , train loss: 6.688 , validation loss: 6.794\n","Epoch: 797 , train loss: 6.810 , validation loss: 6.803\n","Epoch: 798 , train loss: 6.765 , validation loss: 6.783\n","Epoch: 799 , train loss: 6.801 , validation loss: 6.821\n","Epoch: 800 , train loss: 6.776 , validation loss: 6.739\n","Epoch: 801 , train loss: 6.743 , validation loss: 6.688\n","Epoch: 802 , train loss: 6.649 , validation loss: 6.733\n","Epoch: 803 , train loss: 6.702 , validation loss: 6.797\n","Epoch: 804 , train loss: 6.852 , validation loss: 6.717\n","Epoch: 805 , train loss: 6.737 , validation loss: 6.735\n","Epoch: 806 , train loss: 6.707 , validation loss: 6.682\n","Epoch: 807 , train loss: 6.743 , validation loss: 6.808\n","Epoch: 808 , train loss: 6.667 , validation loss: 6.653\n","Epoch: 809 , train loss: 6.783 , validation loss: 6.946\n","Epoch: 810 , train loss: 6.831 , validation loss: 6.657\n","Epoch: 811 , train loss: 6.725 , validation loss: 6.844\n","Epoch: 812 , train loss: 6.798 , validation loss: 6.646\n","Epoch: 813 , train loss: 6.714 , validation loss: 6.676\n","Epoch: 814 , train loss: 6.721 , validation loss: 6.843\n","Epoch: 815 , train loss: 6.688 , validation loss: 6.902\n","Epoch: 816 , train loss: 6.748 , validation loss: 6.688\n","Epoch: 817 , train loss: 6.839 , validation loss: 6.816\n","Epoch: 818 , train loss: 6.607 , validation loss: 6.818\n","Epoch: 819 , train loss: 6.693 , validation loss: 6.820\n","Epoch: 820 , train loss: 6.729 , validation loss: 6.787\n","Epoch: 821 , train loss: 6.828 , validation loss: 6.838\n","Epoch: 822 , train loss: 6.723 , validation loss: 6.802\n","Epoch: 823 , train loss: 6.643 , validation loss: 6.808\n","Epoch: 824 , train loss: 6.678 , validation loss: 6.749\n","Epoch: 825 , train loss: 6.737 , validation loss: 6.724\n","Epoch: 826 , train loss: 6.765 , validation loss: 6.736\n","Epoch: 827 , train loss: 6.752 , validation loss: 6.605\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 828 , train loss: 6.815 , validation loss: 6.677\n","Epoch: 829 , train loss: 6.791 , validation loss: 6.765\n","Epoch: 830 , train loss: 6.861 , validation loss: 6.779\n","Epoch: 831 , train loss: 6.855 , validation loss: 6.863\n","Epoch: 832 , train loss: 6.614 , validation loss: 6.752\n","Epoch: 833 , train loss: 6.553 , validation loss: 6.740\n","Epoch: 834 , train loss: 6.821 , validation loss: 6.811\n","Epoch: 835 , train loss: 6.738 , validation loss: 6.614\n","Epoch: 836 , train loss: 6.775 , validation loss: 6.721\n","Epoch: 837 , train loss: 6.703 , validation loss: 6.685\n","Epoch: 838 , train loss: 6.645 , validation loss: 6.721\n","Epoch: 839 , train loss: 6.730 , validation loss: 6.771\n","Epoch: 840 , train loss: 6.819 , validation loss: 6.715\n","Epoch: 841 , train loss: 6.690 , validation loss: 6.841\n","Epoch: 842 , train loss: 6.648 , validation loss: 6.709\n","Epoch: 843 , train loss: 6.912 , validation loss: 6.682\n","Epoch: 844 , train loss: 6.701 , validation loss: 6.715\n","Epoch: 845 , train loss: 6.773 , validation loss: 6.715\n","Epoch: 846 , train loss: 6.775 , validation loss: 6.671\n","Epoch: 847 , train loss: 6.804 , validation loss: 6.719\n","Epoch: 848 , train loss: 6.650 , validation loss: 6.694\n","Epoch: 849 , train loss: 6.671 , validation loss: 6.691\n","Epoch: 850 , train loss: 6.895 , validation loss: 6.643\n","Epoch: 851 , train loss: 6.575 , validation loss: 6.682\n","Epoch: 852 , train loss: 6.771 , validation loss: 6.793\n","Epoch: 853 , train loss: 6.801 , validation loss: 6.803\n","Epoch: 854 , train loss: 6.705 , validation loss: 6.654\n","Epoch: 855 , train loss: 6.687 , validation loss: 6.754\n","Epoch: 856 , train loss: 6.833 , validation loss: 6.658\n","Epoch: 857 , train loss: 6.531 , validation loss: 6.720\n","Epoch: 858 , train loss: 6.753 , validation loss: 6.720\n","Epoch: 859 , train loss: 6.610 , validation loss: 6.728\n","Epoch: 860 , train loss: 6.972 , validation loss: 6.634\n","Epoch: 861 , train loss: 6.734 , validation loss: 6.640\n","Epoch: 862 , train loss: 6.702 , validation loss: 6.721\n","Epoch: 863 , train loss: 6.755 , validation loss: 6.734\n","Epoch: 864 , train loss: 6.783 , validation loss: 6.684\n","Epoch: 865 , train loss: 6.662 , validation loss: 6.830\n","Epoch: 866 , train loss: 6.663 , validation loss: 6.651\n","Epoch: 867 , train loss: 6.673 , validation loss: 6.666\n","Epoch: 868 , train loss: 6.725 , validation loss: 6.730\n","Epoch: 869 , train loss: 6.748 , validation loss: 6.756\n","Epoch: 870 , train loss: 6.736 , validation loss: 6.673\n","Epoch: 871 , train loss: 6.672 , validation loss: 6.658\n","Epoch: 872 , train loss: 6.755 , validation loss: 6.780\n","Epoch: 873 , train loss: 6.537 , validation loss: 6.756\n","Epoch: 874 , train loss: 6.774 , validation loss: 6.702\n","Epoch: 875 , train loss: 6.776 , validation loss: 6.683\n","Epoch: 876 , train loss: 6.665 , validation loss: 6.733\n","Epoch: 877 , train loss: 6.556 , validation loss: 6.739\n","Epoch: 878 , train loss: 6.625 , validation loss: 6.633\n","Epoch: 879 , train loss: 6.697 , validation loss: 6.693\n","Epoch: 880 , train loss: 6.641 , validation loss: 6.714\n","Epoch: 881 , train loss: 6.652 , validation loss: 6.707\n","Epoch: 882 , train loss: 6.647 , validation loss: 6.735\n","Epoch: 883 , train loss: 6.766 , validation loss: 6.847\n","Epoch: 884 , train loss: 6.706 , validation loss: 6.736\n","Epoch: 885 , train loss: 6.701 , validation loss: 6.745\n","Epoch: 886 , train loss: 6.655 , validation loss: 6.728\n","Epoch: 887 , train loss: 6.593 , validation loss: 6.671\n","Epoch: 888 , train loss: 6.770 , validation loss: 6.704\n","Epoch: 889 , train loss: 6.813 , validation loss: 6.729\n","Epoch: 890 , train loss: 6.589 , validation loss: 6.768\n","Epoch: 891 , train loss: 6.738 , validation loss: 6.685\n","Epoch: 892 , train loss: 6.614 , validation loss: 6.859\n","Epoch: 893 , train loss: 6.611 , validation loss: 6.677\n","Epoch: 894 , train loss: 6.717 , validation loss: 6.671\n","Epoch: 895 , train loss: 6.680 , validation loss: 6.763\n","Epoch: 896 , train loss: 6.692 , validation loss: 6.703\n","Epoch: 897 , train loss: 6.609 , validation loss: 6.822\n","Epoch: 898 , train loss: 6.679 , validation loss: 6.706\n","Epoch: 899 , train loss: 6.662 , validation loss: 6.774\n","Epoch: 900 , train loss: 6.719 , validation loss: 6.800\n","Epoch: 901 , train loss: 6.676 , validation loss: 6.689\n","Epoch: 902 , train loss: 6.778 , validation loss: 6.789\n","Epoch: 903 , train loss: 6.633 , validation loss: 6.748\n","Epoch: 904 , train loss: 6.657 , validation loss: 6.618\n","Epoch: 905 , train loss: 6.616 , validation loss: 6.758\n","Epoch: 906 , train loss: 6.683 , validation loss: 6.718\n","Epoch: 907 , train loss: 6.603 , validation loss: 6.641\n","Epoch: 908 , train loss: 6.668 , validation loss: 6.667\n","Epoch: 909 , train loss: 6.632 , validation loss: 6.667\n","Epoch: 910 , train loss: 6.640 , validation loss: 6.781\n","Epoch: 911 , train loss: 6.707 , validation loss: 6.758\n","Epoch: 912 , train loss: 6.804 , validation loss: 6.734\n","Epoch: 913 , train loss: 6.702 , validation loss: 6.707\n","Epoch: 914 , train loss: 6.615 , validation loss: 6.760\n","Epoch: 915 , train loss: 6.481 , validation loss: 6.769\n","Epoch: 916 , train loss: 6.789 , validation loss: 6.802\n","Epoch: 917 , train loss: 6.593 , validation loss: 6.777\n","Epoch: 918 , train loss: 6.576 , validation loss: 6.735\n","Epoch: 919 , train loss: 6.616 , validation loss: 6.664\n","Epoch: 920 , train loss: 6.537 , validation loss: 6.790\n","Epoch: 921 , train loss: 6.670 , validation loss: 6.851\n","Epoch: 922 , train loss: 6.726 , validation loss: 6.755\n","Epoch: 923 , train loss: 6.578 , validation loss: 6.694\n","Epoch: 924 , train loss: 6.542 , validation loss: 6.699\n","Epoch: 925 , train loss: 6.555 , validation loss: 6.751\n","Epoch: 926 , train loss: 6.653 , validation loss: 6.705\n","Epoch: 927 , train loss: 6.752 , validation loss: 6.679\n","Epoch: 928 , train loss: 6.736 , validation loss: 6.677\n","Epoch: 929 , train loss: 6.618 , validation loss: 6.621\n","Epoch: 930 , train loss: 6.582 , validation loss: 6.690\n","Epoch: 931 , train loss: 6.616 , validation loss: 6.705\n","Epoch: 932 , train loss: 6.627 , validation loss: 6.820\n","Epoch: 933 , train loss: 6.611 , validation loss: 6.723\n","Epoch: 934 , train loss: 6.673 , validation loss: 6.610\n","Epoch: 935 , train loss: 6.582 , validation loss: 6.670\n","Epoch: 936 , train loss: 6.657 , validation loss: 6.680\n","Epoch: 937 , train loss: 6.756 , validation loss: 6.694\n","Epoch: 938 , train loss: 6.710 , validation loss: 6.676\n","Epoch: 939 , train loss: 6.679 , validation loss: 6.700\n","Epoch: 940 , train loss: 6.595 , validation loss: 6.790\n","Epoch: 941 , train loss: 6.705 , validation loss: 6.792\n","Epoch: 942 , train loss: 6.474 , validation loss: 6.760\n","Epoch: 943 , train loss: 6.710 , validation loss: 6.662\n","Epoch: 944 , train loss: 6.717 , validation loss: 6.713\n","Epoch: 945 , train loss: 6.539 , validation loss: 6.671\n","Epoch: 946 , train loss: 6.501 , validation loss: 6.654\n","Epoch: 947 , train loss: 6.575 , validation loss: 6.733\n","Epoch: 948 , train loss: 6.591 , validation loss: 6.710\n","Epoch: 949 , train loss: 6.659 , validation loss: 6.715\n","Epoch: 950 , train loss: 6.592 , validation loss: 6.759\n","Epoch: 951 , train loss: 6.607 , validation loss: 6.694\n","Epoch: 952 , train loss: 6.674 , validation loss: 6.702\n","Epoch: 953 , train loss: 6.629 , validation loss: 6.712\n","Epoch: 954 , train loss: 6.664 , validation loss: 6.583\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 955 , train loss: 6.642 , validation loss: 6.647\n","Epoch: 956 , train loss: 6.521 , validation loss: 6.639\n","Epoch: 957 , train loss: 6.566 , validation loss: 6.648\n","Epoch: 958 , train loss: 6.699 , validation loss: 6.701\n","Epoch: 959 , train loss: 6.665 , validation loss: 6.695\n","Epoch: 960 , train loss: 6.628 , validation loss: 6.621\n","Epoch: 961 , train loss: 6.623 , validation loss: 6.685\n","Epoch: 962 , train loss: 6.663 , validation loss: 6.577\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 963 , train loss: 6.671 , validation loss: 6.603\n","Epoch: 964 , train loss: 6.655 , validation loss: 6.644\n","Epoch: 965 , train loss: 6.558 , validation loss: 6.640\n","Epoch: 966 , train loss: 6.459 , validation loss: 6.658\n","Epoch: 967 , train loss: 6.721 , validation loss: 6.640\n","Epoch: 968 , train loss: 6.581 , validation loss: 6.671\n","Epoch: 969 , train loss: 6.488 , validation loss: 6.666\n","Epoch: 970 , train loss: 6.587 , validation loss: 6.594\n","Epoch: 971 , train loss: 6.580 , validation loss: 6.666\n","Epoch: 972 , train loss: 6.646 , validation loss: 6.706\n","Epoch: 973 , train loss: 6.645 , validation loss: 6.606\n","Epoch: 974 , train loss: 6.615 , validation loss: 6.711\n","Epoch: 975 , train loss: 6.539 , validation loss: 6.657\n","Epoch: 976 , train loss: 6.488 , validation loss: 6.696\n","Epoch: 977 , train loss: 6.616 , validation loss: 6.641\n","Epoch: 978 , train loss: 6.649 , validation loss: 6.638\n","Epoch: 979 , train loss: 6.628 , validation loss: 6.721\n","Epoch: 980 , train loss: 6.537 , validation loss: 6.684\n","Epoch: 981 , train loss: 6.637 , validation loss: 6.622\n","Epoch: 982 , train loss: 6.527 , validation loss: 6.671\n","Epoch: 983 , train loss: 6.614 , validation loss: 6.609\n","Epoch: 984 , train loss: 6.513 , validation loss: 6.632\n","Epoch: 985 , train loss: 6.561 , validation loss: 6.749\n","Epoch: 986 , train loss: 6.470 , validation loss: 6.665\n","Epoch: 987 , train loss: 6.655 , validation loss: 6.687\n","Epoch: 988 , train loss: 6.542 , validation loss: 6.634\n","Epoch: 989 , train loss: 6.588 , validation loss: 6.694\n","Epoch: 990 , train loss: 6.500 , validation loss: 6.751\n","Epoch: 991 , train loss: 6.512 , validation loss: 6.648\n","Epoch: 992 , train loss: 6.528 , validation loss: 6.624\n","Epoch: 993 , train loss: 6.527 , validation loss: 6.643\n","Epoch: 994 , train loss: 6.584 , validation loss: 6.729\n","Epoch: 995 , train loss: 6.608 , validation loss: 6.728\n","Epoch: 996 , train loss: 6.570 , validation loss: 6.686\n","Epoch: 997 , train loss: 6.500 , validation loss: 6.651\n","Epoch: 998 , train loss: 6.539 , validation loss: 6.571\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 999 , train loss: 6.666 , validation loss: 6.635\n","Epoch: 1000 , train loss: 6.600 , validation loss: 6.661\n","Epoch: 1001 , train loss: 6.596 , validation loss: 6.599\n","Epoch: 1002 , train loss: 6.576 , validation loss: 6.580\n","Epoch: 1003 , train loss: 6.589 , validation loss: 6.645\n","Epoch: 1004 , train loss: 6.615 , validation loss: 6.683\n","Epoch: 1005 , train loss: 6.506 , validation loss: 6.713\n","Epoch: 1006 , train loss: 6.493 , validation loss: 6.630\n","Epoch: 1007 , train loss: 6.432 , validation loss: 6.636\n","Epoch: 1008 , train loss: 6.514 , validation loss: 6.728\n","Epoch: 1009 , train loss: 6.577 , validation loss: 6.698\n","Epoch: 1010 , train loss: 6.501 , validation loss: 6.668\n","Epoch: 1011 , train loss: 6.606 , validation loss: 6.700\n","Epoch: 1012 , train loss: 6.457 , validation loss: 6.699\n","Epoch: 1013 , train loss: 6.684 , validation loss: 6.675\n","Epoch: 1014 , train loss: 6.430 , validation loss: 6.677\n","Epoch: 1015 , train loss: 6.601 , validation loss: 6.688\n","Epoch: 1016 , train loss: 6.615 , validation loss: 6.655\n","Epoch: 1017 , train loss: 6.460 , validation loss: 6.657\n","Epoch: 1018 , train loss: 6.573 , validation loss: 6.646\n","Epoch: 1019 , train loss: 6.527 , validation loss: 6.667\n","Epoch: 1020 , train loss: 6.515 , validation loss: 6.753\n","Epoch: 1021 , train loss: 6.531 , validation loss: 6.698\n","Epoch: 1022 , train loss: 6.518 , validation loss: 6.635\n","Epoch: 1023 , train loss: 6.538 , validation loss: 6.542\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1024 , train loss: 6.628 , validation loss: 6.606\n","Epoch: 1025 , train loss: 6.490 , validation loss: 6.597\n","Epoch: 1026 , train loss: 6.483 , validation loss: 6.709\n","Epoch: 1027 , train loss: 6.607 , validation loss: 6.615\n","Epoch: 1028 , train loss: 6.547 , validation loss: 6.611\n","Epoch: 1029 , train loss: 6.558 , validation loss: 6.574\n","Epoch: 1030 , train loss: 6.577 , validation loss: 6.557\n","Epoch: 1031 , train loss: 6.476 , validation loss: 6.714\n","Epoch: 1032 , train loss: 6.613 , validation loss: 6.627\n","Epoch: 1033 , train loss: 6.529 , validation loss: 6.594\n","Epoch: 1034 , train loss: 6.328 , validation loss: 6.576\n","Epoch: 1035 , train loss: 6.477 , validation loss: 6.592\n","Epoch: 1036 , train loss: 6.653 , validation loss: 6.587\n","Epoch: 1037 , train loss: 6.470 , validation loss: 6.648\n","Epoch: 1038 , train loss: 6.628 , validation loss: 6.618\n","Epoch: 1039 , train loss: 6.482 , validation loss: 6.640\n","Epoch: 1040 , train loss: 6.497 , validation loss: 6.680\n","Epoch: 1041 , train loss: 6.438 , validation loss: 6.635\n","Epoch: 1042 , train loss: 6.663 , validation loss: 6.531\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1043 , train loss: 6.519 , validation loss: 6.759\n","Epoch: 1044 , train loss: 6.481 , validation loss: 6.751\n","Epoch: 1045 , train loss: 6.508 , validation loss: 6.744\n","Epoch: 1046 , train loss: 6.572 , validation loss: 6.730\n","Epoch: 1047 , train loss: 6.551 , validation loss: 6.730\n","Epoch: 1048 , train loss: 6.430 , validation loss: 6.627\n","Epoch: 1049 , train loss: 6.592 , validation loss: 6.585\n","Epoch: 1050 , train loss: 6.425 , validation loss: 6.731\n","Epoch: 1051 , train loss: 6.610 , validation loss: 6.646\n","Epoch: 1052 , train loss: 6.439 , validation loss: 6.715\n","Epoch: 1053 , train loss: 6.495 , validation loss: 6.705\n","Epoch: 1054 , train loss: 6.602 , validation loss: 6.537\n","Epoch: 1055 , train loss: 6.471 , validation loss: 6.653\n","Epoch: 1056 , train loss: 6.450 , validation loss: 6.617\n","Epoch: 1057 , train loss: 6.580 , validation loss: 6.623\n","Epoch: 1058 , train loss: 6.451 , validation loss: 6.665\n","Epoch: 1059 , train loss: 6.643 , validation loss: 6.591\n","Epoch: 1060 , train loss: 6.559 , validation loss: 6.674\n","Epoch: 1061 , train loss: 6.434 , validation loss: 6.673\n","Epoch: 1062 , train loss: 6.629 , validation loss: 6.667\n","Epoch: 1063 , train loss: 6.467 , validation loss: 6.654\n","Epoch: 1064 , train loss: 6.504 , validation loss: 6.676\n","Epoch: 1065 , train loss: 6.571 , validation loss: 6.687\n","Epoch: 1066 , train loss: 6.569 , validation loss: 6.752\n","Epoch: 1067 , train loss: 6.397 , validation loss: 6.639\n","Epoch: 1068 , train loss: 6.463 , validation loss: 6.711\n","Epoch: 1069 , train loss: 6.422 , validation loss: 6.640\n","Epoch: 1070 , train loss: 6.580 , validation loss: 6.681\n","Epoch: 1071 , train loss: 6.423 , validation loss: 6.859\n","Epoch: 1072 , train loss: 6.516 , validation loss: 6.645\n","Epoch: 1073 , train loss: 6.597 , validation loss: 6.673\n","Epoch: 1074 , train loss: 6.446 , validation loss: 6.660\n","Epoch: 1075 , train loss: 6.486 , validation loss: 6.770\n","Epoch: 1076 , train loss: 6.455 , validation loss: 6.575\n","Epoch: 1077 , train loss: 6.600 , validation loss: 6.627\n","Epoch: 1078 , train loss: 6.462 , validation loss: 6.711\n","Epoch: 1079 , train loss: 6.574 , validation loss: 6.615\n","Epoch: 1080 , train loss: 6.576 , validation loss: 6.633\n","Epoch: 1081 , train loss: 6.611 , validation loss: 6.615\n","Epoch: 1082 , train loss: 6.484 , validation loss: 6.625\n","Epoch: 1083 , train loss: 6.397 , validation loss: 6.656\n","Epoch: 1084 , train loss: 6.440 , validation loss: 6.591\n","Epoch: 1085 , train loss: 6.506 , validation loss: 6.682\n","Epoch: 1086 , train loss: 6.622 , validation loss: 6.613\n","Epoch: 1087 , train loss: 6.434 , validation loss: 6.620\n","Epoch: 1088 , train loss: 6.469 , validation loss: 6.617\n","Epoch: 1089 , train loss: 6.515 , validation loss: 6.604\n","Epoch: 1090 , train loss: 6.495 , validation loss: 6.746\n","Epoch: 1091 , train loss: 6.624 , validation loss: 6.655\n","Epoch: 1092 , train loss: 6.419 , validation loss: 6.660\n","Epoch: 1093 , train loss: 6.468 , validation loss: 6.673\n","Epoch: 1094 , train loss: 6.526 , validation loss: 6.667\n","Epoch: 1095 , train loss: 6.557 , validation loss: 6.598\n","Epoch: 1096 , train loss: 6.507 , validation loss: 6.701\n","Epoch: 1097 , train loss: 6.525 , validation loss: 6.668\n","Epoch: 1098 , train loss: 6.624 , validation loss: 6.619\n","Epoch: 1099 , train loss: 6.536 , validation loss: 6.606\n","Epoch: 1100 , train loss: 6.469 , validation loss: 6.681\n","Epoch: 1101 , train loss: 6.504 , validation loss: 6.594\n","Epoch: 1102 , train loss: 6.497 , validation loss: 6.668\n","Epoch: 1103 , train loss: 6.411 , validation loss: 6.590\n","Epoch: 1104 , train loss: 6.355 , validation loss: 6.579\n","Epoch: 1105 , train loss: 6.509 , validation loss: 6.735\n","Epoch: 1106 , train loss: 6.516 , validation loss: 6.670\n","Epoch: 1107 , train loss: 6.518 , validation loss: 6.725\n","Epoch: 1108 , train loss: 6.530 , validation loss: 6.746\n","Epoch: 1109 , train loss: 6.404 , validation loss: 6.718\n","Epoch: 1110 , train loss: 6.477 , validation loss: 6.706\n","Epoch: 1111 , train loss: 6.416 , validation loss: 6.600\n","Epoch: 1112 , train loss: 6.604 , validation loss: 6.639\n","Epoch: 1113 , train loss: 6.237 , validation loss: 6.756\n","Epoch: 1114 , train loss: 6.538 , validation loss: 6.652\n","Epoch: 1115 , train loss: 6.298 , validation loss: 6.697\n","Epoch: 1116 , train loss: 6.489 , validation loss: 6.631\n","Epoch: 1117 , train loss: 6.563 , validation loss: 6.583\n","Epoch: 1118 , train loss: 6.430 , validation loss: 6.618\n","Epoch: 1119 , train loss: 6.471 , validation loss: 6.713\n","Epoch: 1120 , train loss: 6.647 , validation loss: 6.561\n","Epoch: 1121 , train loss: 6.504 , validation loss: 6.578\n","Epoch: 1122 , train loss: 6.392 , validation loss: 6.635\n","Epoch: 1123 , train loss: 6.443 , validation loss: 6.663\n","Epoch: 1124 , train loss: 6.543 , validation loss: 6.593\n","Epoch: 1125 , train loss: 6.493 , validation loss: 6.642\n","Epoch: 1126 , train loss: 6.410 , validation loss: 6.636\n","Epoch: 1127 , train loss: 6.333 , validation loss: 6.513\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1128 , train loss: 6.454 , validation loss: 6.638\n","Epoch: 1129 , train loss: 6.399 , validation loss: 6.627\n","Epoch: 1130 , train loss: 6.394 , validation loss: 6.612\n","Epoch: 1131 , train loss: 6.640 , validation loss: 6.577\n","Epoch: 1132 , train loss: 6.459 , validation loss: 6.494\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1133 , train loss: 6.493 , validation loss: 6.646\n","Epoch: 1134 , train loss: 6.491 , validation loss: 6.581\n","Epoch: 1135 , train loss: 6.570 , validation loss: 6.669\n","Epoch: 1136 , train loss: 6.464 , validation loss: 6.670\n","Epoch: 1137 , train loss: 6.437 , validation loss: 6.659\n","Epoch: 1138 , train loss: 6.427 , validation loss: 6.575\n","Epoch: 1139 , train loss: 6.469 , validation loss: 6.573\n","Epoch: 1140 , train loss: 6.404 , validation loss: 6.567\n","Epoch: 1141 , train loss: 6.489 , validation loss: 6.582\n","Epoch: 1142 , train loss: 6.483 , validation loss: 6.600\n","Epoch: 1143 , train loss: 6.423 , validation loss: 6.629\n","Epoch: 1144 , train loss: 6.429 , validation loss: 6.688\n","Epoch: 1145 , train loss: 6.574 , validation loss: 6.504\n","Epoch: 1146 , train loss: 6.316 , validation loss: 6.618\n","Epoch: 1147 , train loss: 6.435 , validation loss: 6.567\n","Epoch: 1148 , train loss: 6.416 , validation loss: 6.603\n","Epoch: 1149 , train loss: 6.462 , validation loss: 6.560\n","Epoch: 1150 , train loss: 6.520 , validation loss: 6.570\n","Epoch: 1151 , train loss: 6.324 , validation loss: 6.556\n","Epoch: 1152 , train loss: 6.572 , validation loss: 6.631\n","Epoch: 1153 , train loss: 6.505 , validation loss: 6.612\n","Epoch: 1154 , train loss: 6.439 , validation loss: 6.522\n","Epoch: 1155 , train loss: 6.362 , validation loss: 6.575\n","Epoch: 1156 , train loss: 6.389 , validation loss: 6.753\n","Epoch: 1157 , train loss: 6.535 , validation loss: 6.582\n","Epoch: 1158 , train loss: 6.495 , validation loss: 6.556\n","Epoch: 1159 , train loss: 6.420 , validation loss: 6.580\n","Epoch: 1160 , train loss: 6.441 , validation loss: 6.548\n","Epoch: 1161 , train loss: 6.425 , validation loss: 6.653\n","Epoch: 1162 , train loss: 6.285 , validation loss: 6.608\n","Epoch: 1163 , train loss: 6.576 , validation loss: 6.689\n","Epoch: 1164 , train loss: 6.415 , validation loss: 6.531\n","Epoch: 1165 , train loss: 6.356 , validation loss: 6.577\n","Epoch: 1166 , train loss: 6.543 , validation loss: 6.590\n","Epoch: 1167 , train loss: 6.475 , validation loss: 6.630\n","Epoch: 1168 , train loss: 6.458 , validation loss: 6.575\n","Epoch: 1169 , train loss: 6.418 , validation loss: 6.675\n","Epoch: 1170 , train loss: 6.494 , validation loss: 6.511\n","Epoch: 1171 , train loss: 6.548 , validation loss: 6.570\n","Epoch: 1172 , train loss: 6.304 , validation loss: 6.586\n","Epoch: 1173 , train loss: 6.398 , validation loss: 6.497\n","Epoch: 1174 , train loss: 6.481 , validation loss: 6.585\n","Epoch: 1175 , train loss: 6.476 , validation loss: 6.559\n","Epoch: 1176 , train loss: 6.398 , validation loss: 6.592\n","Epoch: 1177 , train loss: 6.443 , validation loss: 6.633\n","Epoch: 1178 , train loss: 6.383 , validation loss: 6.560\n","Epoch: 1179 , train loss: 6.402 , validation loss: 6.592\n","Epoch: 1180 , train loss: 6.623 , validation loss: 6.590\n","Epoch: 1181 , train loss: 6.328 , validation loss: 6.582\n","Epoch: 1182 , train loss: 6.466 , validation loss: 6.595\n","Epoch: 1183 , train loss: 6.345 , validation loss: 6.500\n","Epoch: 1184 , train loss: 6.510 , validation loss: 6.643\n","Epoch: 1185 , train loss: 6.530 , validation loss: 6.637\n","Epoch: 1186 , train loss: 6.396 , validation loss: 6.648\n","Epoch: 1187 , train loss: 6.421 , validation loss: 6.591\n","Epoch: 1188 , train loss: 6.384 , validation loss: 6.628\n","Epoch: 1189 , train loss: 6.340 , validation loss: 6.622\n","Epoch: 1190 , train loss: 6.429 , validation loss: 6.694\n","Epoch: 1191 , train loss: 6.378 , validation loss: 6.657\n","Epoch: 1192 , train loss: 6.356 , validation loss: 6.619\n","Epoch: 1193 , train loss: 6.307 , validation loss: 6.585\n","Epoch: 1194 , train loss: 6.317 , validation loss: 6.605\n","Epoch: 1195 , train loss: 6.391 , validation loss: 6.559\n","Epoch: 1196 , train loss: 6.253 , validation loss: 6.632\n","Epoch: 1197 , train loss: 6.313 , validation loss: 6.537\n","Epoch: 1198 , train loss: 6.419 , validation loss: 6.594\n","Epoch: 1199 , train loss: 6.319 , validation loss: 6.633\n","Epoch: 1200 , train loss: 6.361 , validation loss: 6.573\n","Epoch: 1201 , train loss: 6.357 , validation loss: 6.546\n","Epoch: 1202 , train loss: 6.461 , validation loss: 6.593\n","Epoch: 1203 , train loss: 6.371 , validation loss: 6.605\n","Epoch: 1204 , train loss: 6.376 , validation loss: 6.625\n","Epoch: 1205 , train loss: 6.378 , validation loss: 6.661\n","Epoch: 1206 , train loss: 6.349 , validation loss: 6.629\n","Epoch: 1207 , train loss: 6.419 , validation loss: 6.597\n","Epoch: 1208 , train loss: 6.426 , validation loss: 6.596\n","Epoch: 1209 , train loss: 6.366 , validation loss: 6.539\n","Epoch: 1210 , train loss: 6.406 , validation loss: 6.649\n","Epoch: 1211 , train loss: 6.312 , validation loss: 6.603\n","Epoch: 1212 , train loss: 6.394 , validation loss: 6.549\n","Epoch: 1213 , train loss: 6.445 , validation loss: 6.641\n","Epoch: 1214 , train loss: 6.378 , validation loss: 6.573\n","Epoch: 1215 , train loss: 6.331 , validation loss: 6.618\n","Epoch: 1216 , train loss: 6.423 , validation loss: 6.527\n","Epoch: 1217 , train loss: 6.407 , validation loss: 6.532\n","Epoch: 1218 , train loss: 6.435 , validation loss: 6.460\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1219 , train loss: 6.392 , validation loss: 6.584\n","Epoch: 1220 , train loss: 6.422 , validation loss: 6.620\n","Epoch: 1221 , train loss: 6.376 , validation loss: 6.600\n","Epoch: 1222 , train loss: 6.544 , validation loss: 6.529\n","Epoch: 1223 , train loss: 6.264 , validation loss: 6.502\n","Epoch: 1224 , train loss: 6.323 , validation loss: 6.613\n","Epoch: 1225 , train loss: 6.544 , validation loss: 6.588\n","Epoch: 1226 , train loss: 6.355 , validation loss: 6.584\n","Epoch: 1227 , train loss: 6.464 , validation loss: 6.605\n","Epoch: 1228 , train loss: 6.365 , validation loss: 6.590\n","Epoch: 1229 , train loss: 6.414 , validation loss: 6.554\n","Epoch: 1230 , train loss: 6.379 , validation loss: 6.556\n","Epoch: 1231 , train loss: 6.458 , validation loss: 6.598\n","Epoch: 1232 , train loss: 6.409 , validation loss: 6.563\n","Epoch: 1233 , train loss: 6.421 , validation loss: 6.613\n","Epoch: 1234 , train loss: 6.503 , validation loss: 6.517\n","Epoch: 1235 , train loss: 6.462 , validation loss: 6.587\n","Epoch: 1236 , train loss: 6.327 , validation loss: 6.579\n","Epoch: 1237 , train loss: 6.411 , validation loss: 6.673\n","Epoch: 1238 , train loss: 6.291 , validation loss: 6.609\n","Epoch: 1239 , train loss: 6.332 , validation loss: 6.612\n","Epoch: 1240 , train loss: 6.383 , validation loss: 6.657\n","Epoch: 1241 , train loss: 6.338 , validation loss: 6.705\n","Epoch: 1242 , train loss: 6.360 , validation loss: 6.559\n","Epoch: 1243 , train loss: 6.377 , validation loss: 6.634\n","Epoch: 1244 , train loss: 6.467 , validation loss: 6.635\n","Epoch: 1245 , train loss: 6.482 , validation loss: 6.599\n","Epoch: 1246 , train loss: 6.399 , validation loss: 6.556\n","Epoch: 1247 , train loss: 6.499 , validation loss: 6.601\n","Epoch: 1248 , train loss: 6.361 , validation loss: 6.597\n","Epoch: 1249 , train loss: 6.393 , validation loss: 6.491\n","Epoch: 1250 , train loss: 6.358 , validation loss: 6.577\n","Epoch: 1251 , train loss: 6.406 , validation loss: 6.523\n","Epoch: 1252 , train loss: 6.421 , validation loss: 6.571\n","Epoch: 1253 , train loss: 6.368 , validation loss: 6.552\n","Epoch: 1254 , train loss: 6.380 , validation loss: 6.496\n","Epoch: 1255 , train loss: 6.407 , validation loss: 6.647\n","Epoch: 1256 , train loss: 6.312 , validation loss: 6.648\n","Epoch: 1257 , train loss: 6.365 , validation loss: 6.576\n","Epoch: 1258 , train loss: 6.463 , validation loss: 6.592\n","Epoch: 1259 , train loss: 6.324 , validation loss: 6.600\n","Epoch: 1260 , train loss: 6.410 , validation loss: 6.653\n","Epoch: 1261 , train loss: 6.289 , validation loss: 6.630\n","Epoch: 1262 , train loss: 6.342 , validation loss: 6.578\n","Epoch: 1263 , train loss: 6.244 , validation loss: 6.683\n","Epoch: 1264 , train loss: 6.302 , validation loss: 6.612\n","Epoch: 1265 , train loss: 6.389 , validation loss: 6.596\n","Epoch: 1266 , train loss: 6.370 , validation loss: 6.633\n","Epoch: 1267 , train loss: 6.266 , validation loss: 6.529\n","Epoch: 1268 , train loss: 6.336 , validation loss: 6.615\n","Epoch: 1269 , train loss: 6.367 , validation loss: 6.585\n","Epoch: 1270 , train loss: 6.367 , validation loss: 6.539\n","Epoch: 1271 , train loss: 6.390 , validation loss: 6.546\n","Epoch: 1272 , train loss: 6.435 , validation loss: 6.618\n","Epoch: 1273 , train loss: 6.335 , validation loss: 6.559\n","Epoch: 1274 , train loss: 6.380 , validation loss: 6.667\n","Epoch: 1275 , train loss: 6.306 , validation loss: 6.625\n","Epoch: 1276 , train loss: 6.319 , validation loss: 6.638\n","Epoch: 1277 , train loss: 6.277 , validation loss: 6.622\n","Epoch: 1278 , train loss: 6.420 , validation loss: 6.557\n","Epoch: 1279 , train loss: 6.388 , validation loss: 6.632\n","Epoch: 1280 , train loss: 6.357 , validation loss: 6.614\n","Epoch: 1281 , train loss: 6.334 , validation loss: 6.636\n","Epoch: 1282 , train loss: 6.350 , validation loss: 6.659\n","Epoch: 1283 , train loss: 6.186 , validation loss: 6.567\n","Epoch: 1284 , train loss: 6.373 , validation loss: 6.585\n","Epoch: 1285 , train loss: 6.309 , validation loss: 6.550\n","Epoch: 1286 , train loss: 6.346 , validation loss: 6.572\n","Epoch: 1287 , train loss: 6.379 , validation loss: 6.582\n","Epoch: 1288 , train loss: 6.285 , validation loss: 6.642\n","Epoch: 1289 , train loss: 6.402 , validation loss: 6.585\n","Epoch: 1290 , train loss: 6.366 , validation loss: 6.635\n","Epoch: 1291 , train loss: 6.434 , validation loss: 6.658\n","Epoch: 1292 , train loss: 6.368 , validation loss: 6.745\n","Epoch: 1293 , train loss: 6.201 , validation loss: 6.767\n","Epoch: 1294 , train loss: 6.318 , validation loss: 6.661\n","Epoch: 1295 , train loss: 6.319 , validation loss: 6.649\n","Epoch: 1296 , train loss: 6.273 , validation loss: 6.663\n","Epoch: 1297 , train loss: 6.360 , validation loss: 6.678\n","Epoch: 1298 , train loss: 6.314 , validation loss: 6.637\n","Epoch: 1299 , train loss: 6.352 , validation loss: 6.604\n","Epoch: 1300 , train loss: 6.253 , validation loss: 6.630\n","Epoch: 1301 , train loss: 6.313 , validation loss: 6.676\n","Epoch: 1302 , train loss: 6.360 , validation loss: 6.603\n","Epoch: 1303 , train loss: 6.335 , validation loss: 6.588\n","Epoch: 1304 , train loss: 6.275 , validation loss: 6.612\n","Epoch: 1305 , train loss: 6.357 , validation loss: 6.513\n","Epoch: 1306 , train loss: 6.396 , validation loss: 6.566\n","Epoch: 1307 , train loss: 6.354 , validation loss: 6.612\n","Epoch: 1308 , train loss: 6.461 , validation loss: 6.567\n","Epoch: 1309 , train loss: 6.283 , validation loss: 6.666\n","Epoch: 1310 , train loss: 6.475 , validation loss: 6.550\n","Epoch: 1311 , train loss: 6.336 , validation loss: 6.578\n","Epoch: 1312 , train loss: 6.384 , validation loss: 6.634\n","Epoch: 1313 , train loss: 6.226 , validation loss: 6.570\n","Epoch: 1314 , train loss: 6.478 , validation loss: 6.677\n","Epoch: 1315 , train loss: 6.399 , validation loss: 6.581\n","Epoch: 1316 , train loss: 6.338 , validation loss: 6.690\n","Epoch: 1317 , train loss: 6.295 , validation loss: 6.750\n","Epoch: 1318 , train loss: 6.240 , validation loss: 6.685\n","Epoch: 1319 , train loss: 6.320 , validation loss: 6.659\n","Epoch: 1320 , train loss: 6.328 , validation loss: 6.592\n","Epoch: 1321 , train loss: 6.324 , validation loss: 6.597\n","Epoch: 1322 , train loss: 6.123 , validation loss: 6.571\n","Epoch: 1323 , train loss: 6.379 , validation loss: 6.648\n","Epoch: 1324 , train loss: 6.320 , validation loss: 6.626\n","Epoch: 1325 , train loss: 6.364 , validation loss: 6.667\n","Epoch: 1326 , train loss: 6.307 , validation loss: 6.618\n","Epoch: 1327 , train loss: 6.264 , validation loss: 6.600\n","Epoch: 1328 , train loss: 6.296 , validation loss: 6.710\n","Epoch: 1329 , train loss: 6.420 , validation loss: 6.640\n","Epoch: 1330 , train loss: 6.243 , validation loss: 6.626\n","Epoch: 1331 , train loss: 6.308 , validation loss: 6.587\n","Epoch: 1332 , train loss: 6.336 , validation loss: 6.576\n","Epoch: 1333 , train loss: 6.339 , validation loss: 6.587\n","Epoch: 1334 , train loss: 6.229 , validation loss: 6.580\n","Epoch: 1335 , train loss: 6.301 , validation loss: 6.566\n","Epoch: 1336 , train loss: 6.277 , validation loss: 6.630\n","Epoch: 1337 , train loss: 6.332 , validation loss: 6.645\n","Epoch: 1338 , train loss: 6.233 , validation loss: 6.652\n","Epoch: 1339 , train loss: 6.353 , validation loss: 6.605\n","Epoch: 1340 , train loss: 6.215 , validation loss: 6.597\n","Epoch: 1341 , train loss: 6.285 , validation loss: 6.599\n","Epoch: 1342 , train loss: 6.161 , validation loss: 6.787\n","Epoch: 1343 , train loss: 6.355 , validation loss: 6.539\n","Epoch: 1344 , train loss: 6.237 , validation loss: 6.590\n","Epoch: 1345 , train loss: 6.324 , validation loss: 6.602\n","Epoch: 1346 , train loss: 6.303 , validation loss: 6.521\n","Epoch: 1347 , train loss: 6.341 , validation loss: 6.720\n","Epoch: 1348 , train loss: 6.326 , validation loss: 6.595\n","Epoch: 1349 , train loss: 6.232 , validation loss: 6.629\n","Epoch: 1350 , train loss: 6.259 , validation loss: 6.513\n","Epoch: 1351 , train loss: 6.349 , validation loss: 6.561\n","Epoch: 1352 , train loss: 6.312 , validation loss: 6.611\n","Epoch: 1353 , train loss: 6.224 , validation loss: 6.504\n","Epoch: 1354 , train loss: 6.379 , validation loss: 6.628\n","Epoch: 1355 , train loss: 6.215 , validation loss: 6.455\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1356 , train loss: 6.386 , validation loss: 6.523\n","Epoch: 1357 , train loss: 6.368 , validation loss: 6.622\n","Epoch: 1358 , train loss: 6.270 , validation loss: 6.566\n","Epoch: 1359 , train loss: 6.325 , validation loss: 6.523\n","Epoch: 1360 , train loss: 6.365 , validation loss: 6.552\n","Epoch: 1361 , train loss: 6.303 , validation loss: 6.546\n","Epoch: 1362 , train loss: 6.223 , validation loss: 6.542\n","Epoch: 1363 , train loss: 6.321 , validation loss: 6.544\n","Epoch: 1364 , train loss: 6.237 , validation loss: 6.546\n","Epoch: 1365 , train loss: 6.273 , validation loss: 6.559\n","Epoch: 1366 , train loss: 6.216 , validation loss: 6.538\n","Epoch: 1367 , train loss: 6.428 , validation loss: 6.675\n","Epoch: 1368 , train loss: 6.157 , validation loss: 6.524\n","Epoch: 1369 , train loss: 6.307 , validation loss: 6.596\n","Epoch: 1370 , train loss: 6.409 , validation loss: 6.607\n","Epoch: 1371 , train loss: 6.287 , validation loss: 6.624\n","Epoch: 1372 , train loss: 6.301 , validation loss: 6.614\n","Epoch: 1373 , train loss: 6.311 , validation loss: 6.600\n","Epoch: 1374 , train loss: 6.164 , validation loss: 6.626\n","Epoch: 1375 , train loss: 6.288 , validation loss: 6.563\n","Epoch: 1376 , train loss: 6.263 , validation loss: 6.609\n","Epoch: 1377 , train loss: 6.375 , validation loss: 6.517\n","Epoch: 1378 , train loss: 6.284 , validation loss: 6.740\n","Epoch: 1379 , train loss: 6.299 , validation loss: 6.656\n","Epoch: 1380 , train loss: 6.159 , validation loss: 6.635\n","Epoch: 1381 , train loss: 6.122 , validation loss: 6.615\n","Epoch: 1382 , train loss: 6.286 , validation loss: 6.629\n","Epoch: 1383 , train loss: 6.312 , validation loss: 6.641\n","Epoch: 1384 , train loss: 6.267 , validation loss: 6.594\n","Epoch: 1385 , train loss: 6.250 , validation loss: 6.574\n","Epoch: 1386 , train loss: 6.294 , validation loss: 6.566\n","Epoch: 1387 , train loss: 6.303 , validation loss: 6.587\n","Epoch: 1388 , train loss: 6.322 , validation loss: 6.627\n","Epoch: 1389 , train loss: 6.285 , validation loss: 6.590\n","Epoch: 1390 , train loss: 6.281 , validation loss: 6.676\n","Epoch: 1391 , train loss: 6.329 , validation loss: 6.555\n","Epoch: 1392 , train loss: 6.168 , validation loss: 6.577\n","Epoch: 1393 , train loss: 6.242 , validation loss: 6.569\n","Epoch: 1394 , train loss: 6.230 , validation loss: 6.702\n","Epoch: 1395 , train loss: 6.412 , validation loss: 6.604\n","Epoch: 1396 , train loss: 6.366 , validation loss: 6.541\n","Epoch: 1397 , train loss: 6.222 , validation loss: 6.619\n","Epoch: 1398 , train loss: 6.199 , validation loss: 6.538\n","Epoch: 1399 , train loss: 6.358 , validation loss: 6.549\n","Epoch: 1400 , train loss: 6.246 , validation loss: 6.641\n","Epoch: 1401 , train loss: 6.321 , validation loss: 6.660\n","Epoch: 1402 , train loss: 6.279 , validation loss: 6.583\n","Epoch: 1403 , train loss: 6.235 , validation loss: 6.590\n","Epoch: 1404 , train loss: 6.314 , validation loss: 6.601\n","Epoch: 1405 , train loss: 6.286 , validation loss: 6.604\n","Epoch: 1406 , train loss: 6.245 , validation loss: 6.602\n","Epoch: 1407 , train loss: 6.323 , validation loss: 6.484\n","Epoch: 1408 , train loss: 6.286 , validation loss: 6.521\n","Epoch: 1409 , train loss: 6.154 , validation loss: 6.544\n","Epoch: 1410 , train loss: 6.224 , validation loss: 6.525\n","Epoch: 1411 , train loss: 6.184 , validation loss: 6.538\n","Epoch: 1412 , train loss: 6.174 , validation loss: 6.580\n","Epoch: 1413 , train loss: 6.209 , validation loss: 6.584\n","Epoch: 1414 , train loss: 6.215 , validation loss: 6.596\n","Epoch: 1415 , train loss: 6.285 , validation loss: 6.699\n","Epoch: 1416 , train loss: 6.246 , validation loss: 6.597\n","Epoch: 1417 , train loss: 6.183 , validation loss: 6.655\n","Epoch: 1418 , train loss: 6.349 , validation loss: 6.687\n","Epoch: 1419 , train loss: 6.194 , validation loss: 6.645\n","Epoch: 1420 , train loss: 6.365 , validation loss: 6.631\n","Epoch: 1421 , train loss: 6.286 , validation loss: 6.565\n","Epoch: 1422 , train loss: 6.322 , validation loss: 6.583\n","Epoch: 1423 , train loss: 6.135 , validation loss: 6.629\n","Epoch: 1424 , train loss: 6.342 , validation loss: 6.611\n","Epoch: 1425 , train loss: 6.154 , validation loss: 6.575\n","Epoch: 1426 , train loss: 6.232 , validation loss: 6.483\n","Epoch: 1427 , train loss: 6.393 , validation loss: 6.609\n","Epoch: 1428 , train loss: 6.245 , validation loss: 6.533\n","Epoch: 1429 , train loss: 6.184 , validation loss: 6.570\n","Epoch: 1430 , train loss: 6.299 , validation loss: 6.595\n","Epoch: 1431 , train loss: 6.288 , validation loss: 6.562\n","Epoch: 1432 , train loss: 6.360 , validation loss: 6.695\n","Epoch: 1433 , train loss: 6.239 , validation loss: 6.603\n","Epoch: 1434 , train loss: 6.291 , validation loss: 6.634\n","Epoch: 1435 , train loss: 6.296 , validation loss: 6.629\n","Epoch: 1436 , train loss: 6.270 , validation loss: 6.614\n","Epoch: 1437 , train loss: 6.125 , validation loss: 6.563\n","Epoch: 1438 , train loss: 6.191 , validation loss: 6.553\n","Epoch: 1439 , train loss: 6.296 , validation loss: 6.583\n","Epoch: 1440 , train loss: 6.343 , validation loss: 6.559\n","Epoch: 1441 , train loss: 6.261 , validation loss: 6.584\n","Epoch: 1442 , train loss: 6.218 , validation loss: 6.641\n","Epoch: 1443 , train loss: 6.267 , validation loss: 6.628\n","Epoch: 1444 , train loss: 6.118 , validation loss: 6.616\n","Epoch: 1445 , train loss: 6.267 , validation loss: 6.592\n","Epoch: 1446 , train loss: 6.103 , validation loss: 6.557\n","Epoch: 1447 , train loss: 6.219 , validation loss: 6.549\n","Epoch: 1448 , train loss: 6.156 , validation loss: 6.553\n","Epoch: 1449 , train loss: 6.127 , validation loss: 6.491\n","Epoch: 1450 , train loss: 6.428 , validation loss: 6.580\n","Epoch: 1451 , train loss: 6.287 , validation loss: 6.590\n","Epoch: 1452 , train loss: 6.330 , validation loss: 6.583\n","Epoch: 1453 , train loss: 6.098 , validation loss: 6.661\n","Epoch: 1454 , train loss: 6.151 , validation loss: 6.536\n","Epoch: 1455 , train loss: 6.149 , validation loss: 6.568\n","Epoch: 1456 , train loss: 6.215 , validation loss: 6.586\n","Epoch: 1457 , train loss: 6.300 , validation loss: 6.594\n","Epoch: 1458 , train loss: 6.336 , validation loss: 6.557\n","Epoch: 1459 , train loss: 6.169 , validation loss: 6.570\n","Epoch: 1460 , train loss: 6.286 , validation loss: 6.581\n","Epoch: 1461 , train loss: 6.137 , validation loss: 6.560\n","Epoch: 1462 , train loss: 6.290 , validation loss: 6.559\n","Epoch: 1463 , train loss: 6.276 , validation loss: 6.650\n","Epoch: 1464 , train loss: 6.253 , validation loss: 6.538\n","Epoch: 1465 , train loss: 6.202 , validation loss: 6.563\n","Epoch: 1466 , train loss: 6.285 , validation loss: 6.662\n","Epoch: 1467 , train loss: 6.209 , validation loss: 6.629\n","Epoch: 1468 , train loss: 6.168 , validation loss: 6.584\n","Epoch: 1469 , train loss: 6.261 , validation loss: 6.516\n","Epoch: 1470 , train loss: 6.302 , validation loss: 6.517\n","Epoch: 1471 , train loss: 6.253 , validation loss: 6.755\n","Epoch: 1472 , train loss: 6.249 , validation loss: 6.576\n","Epoch: 1473 , train loss: 6.279 , validation loss: 6.577\n","Epoch: 1474 , train loss: 6.387 , validation loss: 6.661\n","Epoch: 1475 , train loss: 6.303 , validation loss: 6.623\n","Epoch: 1476 , train loss: 6.219 , validation loss: 6.628\n","Epoch: 1477 , train loss: 6.292 , validation loss: 6.626\n","Epoch: 1478 , train loss: 6.367 , validation loss: 6.609\n","Epoch: 1479 , train loss: 6.166 , validation loss: 6.628\n","Epoch: 1480 , train loss: 6.273 , validation loss: 6.598\n","Epoch: 1481 , train loss: 6.345 , validation loss: 6.700\n","Epoch: 1482 , train loss: 6.255 , validation loss: 6.661\n","Epoch: 1483 , train loss: 6.307 , validation loss: 6.627\n","Epoch: 1484 , train loss: 6.221 , validation loss: 6.639\n","Epoch: 1485 , train loss: 6.280 , validation loss: 6.676\n","Epoch: 1486 , train loss: 6.322 , validation loss: 6.634\n","Epoch: 1487 , train loss: 6.252 , validation loss: 6.668\n","Epoch: 1488 , train loss: 6.337 , validation loss: 6.650\n","Epoch: 1489 , train loss: 6.229 , validation loss: 6.680\n","Epoch: 1490 , train loss: 6.281 , validation loss: 6.613\n","Epoch: 1491 , train loss: 6.270 , validation loss: 6.638\n","Epoch: 1492 , train loss: 6.199 , validation loss: 6.615\n","Epoch: 1493 , train loss: 6.216 , validation loss: 6.560\n","Epoch: 1494 , train loss: 6.281 , validation loss: 6.658\n","Epoch: 1495 , train loss: 6.162 , validation loss: 6.686\n","Epoch: 1496 , train loss: 6.231 , validation loss: 6.622\n","Epoch: 1497 , train loss: 6.228 , validation loss: 6.570\n","Epoch: 1498 , train loss: 6.241 , validation loss: 6.691\n","Epoch: 1499 , train loss: 6.232 , validation loss: 6.611\n","Epoch: 1500 , train loss: 6.303 , validation loss: 6.621\n","Epoch: 1501 , train loss: 6.160 , validation loss: 6.565\n","Epoch: 1502 , train loss: 6.322 , validation loss: 6.675\n","Epoch: 1503 , train loss: 6.248 , validation loss: 6.648\n","Epoch: 1504 , train loss: 6.238 , validation loss: 6.618\n","Epoch: 1505 , train loss: 6.308 , validation loss: 6.567\n","Epoch: 1506 , train loss: 6.221 , validation loss: 6.625\n","Epoch: 1507 , train loss: 6.297 , validation loss: 6.667\n","Epoch: 1508 , train loss: 6.234 , validation loss: 6.590\n","Epoch: 1509 , train loss: 6.220 , validation loss: 6.674\n","Epoch: 1510 , train loss: 6.179 , validation loss: 6.593\n","Epoch: 1511 , train loss: 6.034 , validation loss: 6.583\n","Epoch: 1512 , train loss: 6.313 , validation loss: 6.620\n","Epoch: 1513 , train loss: 6.256 , validation loss: 6.633\n","Epoch: 1514 , train loss: 6.114 , validation loss: 6.683\n","Epoch: 1515 , train loss: 6.141 , validation loss: 6.647\n","Epoch: 1516 , train loss: 6.169 , validation loss: 6.567\n","Epoch: 1517 , train loss: 6.187 , validation loss: 6.670\n","Epoch: 1518 , train loss: 6.274 , validation loss: 6.639\n","Epoch: 1519 , train loss: 6.252 , validation loss: 6.527\n","Epoch: 1520 , train loss: 6.288 , validation loss: 6.610\n","Epoch: 1521 , train loss: 6.262 , validation loss: 6.639\n","Epoch: 1522 , train loss: 6.165 , validation loss: 6.569\n","Epoch: 1523 , train loss: 6.276 , validation loss: 6.566\n","Epoch: 1524 , train loss: 6.303 , validation loss: 6.527\n","Epoch: 1525 , train loss: 6.221 , validation loss: 6.533\n","Epoch: 1526 , train loss: 6.208 , validation loss: 6.659\n","Epoch: 1527 , train loss: 6.171 , validation loss: 6.642\n","Epoch: 1528 , train loss: 6.183 , validation loss: 6.495\n","Epoch: 1529 , train loss: 6.209 , validation loss: 6.590\n","Epoch: 1530 , train loss: 6.264 , validation loss: 6.674\n","Epoch: 1531 , train loss: 6.177 , validation loss: 6.704\n","Epoch: 1532 , train loss: 6.103 , validation loss: 6.551\n","Epoch: 1533 , train loss: 6.151 , validation loss: 6.594\n","Epoch: 1534 , train loss: 6.210 , validation loss: 6.514\n","Epoch: 1535 , train loss: 6.212 , validation loss: 6.618\n","Epoch: 1536 , train loss: 6.145 , validation loss: 6.669\n","Epoch: 1537 , train loss: 6.137 , validation loss: 6.634\n","Epoch: 1538 , train loss: 6.103 , validation loss: 6.666\n","Epoch: 1539 , train loss: 6.226 , validation loss: 6.670\n","Epoch: 1540 , train loss: 6.190 , validation loss: 6.654\n","Epoch: 1541 , train loss: 6.276 , validation loss: 6.667\n","Epoch: 1542 , train loss: 6.195 , validation loss: 6.573\n","Epoch: 1543 , train loss: 6.292 , validation loss: 6.661\n","Epoch: 1544 , train loss: 6.164 , validation loss: 6.637\n","Epoch: 1545 , train loss: 6.104 , validation loss: 6.592\n","Epoch: 1546 , train loss: 6.152 , validation loss: 6.615\n","Epoch: 1547 , train loss: 6.204 , validation loss: 6.587\n","Epoch: 1548 , train loss: 6.165 , validation loss: 6.686\n","Epoch: 1549 , train loss: 6.188 , validation loss: 6.610\n","Epoch: 1550 , train loss: 6.188 , validation loss: 6.592\n","Epoch: 1551 , train loss: 6.096 , validation loss: 6.532\n","Epoch: 1552 , train loss: 6.176 , validation loss: 6.671\n","Epoch: 1553 , train loss: 6.100 , validation loss: 6.645\n","Epoch: 1554 , train loss: 6.117 , validation loss: 6.550\n","Epoch: 1555 , train loss: 6.116 , validation loss: 6.546\n","Epoch: 1556 , train loss: 6.193 , validation loss: 6.675\n","Epoch: 1557 , train loss: 6.144 , validation loss: 6.677\n","Epoch: 1558 , train loss: 6.185 , validation loss: 6.623\n","Epoch: 1559 , train loss: 6.128 , validation loss: 6.548\n","Epoch: 1560 , train loss: 6.196 , validation loss: 6.598\n","Epoch: 1561 , train loss: 6.221 , validation loss: 6.582\n","Epoch: 1562 , train loss: 6.204 , validation loss: 6.640\n","Epoch: 1563 , train loss: 6.159 , validation loss: 6.512\n","Epoch: 1564 , train loss: 6.256 , validation loss: 6.527\n","Epoch: 1565 , train loss: 6.189 , validation loss: 6.568\n","Epoch: 1566 , train loss: 6.114 , validation loss: 6.566\n","Epoch: 1567 , train loss: 6.175 , validation loss: 6.518\n","Epoch: 1568 , train loss: 6.059 , validation loss: 6.603\n","Epoch: 1569 , train loss: 6.124 , validation loss: 6.569\n","Epoch: 1570 , train loss: 6.160 , validation loss: 6.612\n","Epoch: 1571 , train loss: 6.171 , validation loss: 6.600\n","Epoch: 1572 , train loss: 6.018 , validation loss: 6.509\n","Epoch: 1573 , train loss: 6.255 , validation loss: 6.482\n","Epoch: 1574 , train loss: 6.287 , validation loss: 6.603\n","Epoch: 1575 , train loss: 6.157 , validation loss: 6.494\n","Epoch: 1576 , train loss: 6.023 , validation loss: 6.471\n","Epoch: 1577 , train loss: 6.072 , validation loss: 6.527\n","Epoch: 1578 , train loss: 6.232 , validation loss: 6.580\n","Epoch: 1579 , train loss: 6.200 , validation loss: 6.584\n","Epoch: 1580 , train loss: 6.033 , validation loss: 6.612\n","Epoch: 1581 , train loss: 6.187 , validation loss: 6.578\n","Epoch: 1582 , train loss: 6.118 , validation loss: 6.590\n","Epoch: 1583 , train loss: 6.129 , validation loss: 6.551\n","Epoch: 1584 , train loss: 6.026 , validation loss: 6.564\n","Epoch: 1585 , train loss: 6.189 , validation loss: 6.626\n","Epoch: 1586 , train loss: 6.077 , validation loss: 6.542\n","Epoch: 1587 , train loss: 6.115 , validation loss: 6.610\n","Epoch: 1588 , train loss: 6.045 , validation loss: 6.580\n","Epoch: 1589 , train loss: 6.327 , validation loss: 6.595\n","Epoch: 1590 , train loss: 6.076 , validation loss: 6.559\n","Epoch: 1591 , train loss: 6.021 , validation loss: 6.575\n","Epoch: 1592 , train loss: 6.211 , validation loss: 6.619\n","Epoch: 1593 , train loss: 6.001 , validation loss: 6.535\n","Epoch: 1594 , train loss: 6.086 , validation loss: 6.525\n","Epoch: 1595 , train loss: 6.247 , validation loss: 6.708\n","Epoch: 1596 , train loss: 6.085 , validation loss: 6.594\n","Epoch: 1597 , train loss: 6.102 , validation loss: 6.555\n","Epoch: 1598 , train loss: 6.245 , validation loss: 6.617\n","Epoch: 1599 , train loss: 6.256 , validation loss: 6.500\n","Epoch: 1600 , train loss: 6.134 , validation loss: 6.565\n","Epoch: 1601 , train loss: 6.137 , validation loss: 6.552\n","Epoch: 1602 , train loss: 6.001 , validation loss: 6.564\n","Epoch: 1603 , train loss: 6.141 , validation loss: 6.622\n","Epoch: 1604 , train loss: 6.090 , validation loss: 6.584\n","Epoch: 1605 , train loss: 6.222 , validation loss: 6.554\n","Epoch: 1606 , train loss: 6.168 , validation loss: 6.528\n","Epoch: 1607 , train loss: 6.268 , validation loss: 6.504\n","Epoch: 1608 , train loss: 6.146 , validation loss: 6.611\n","Epoch: 1609 , train loss: 6.160 , validation loss: 6.574\n","Epoch: 1610 , train loss: 6.235 , validation loss: 6.511\n","Epoch: 1611 , train loss: 6.085 , validation loss: 6.559\n","Epoch: 1612 , train loss: 6.203 , validation loss: 6.501\n","Epoch: 1613 , train loss: 6.197 , validation loss: 6.670\n","Epoch: 1614 , train loss: 6.162 , validation loss: 6.537\n","Epoch: 1615 , train loss: 6.111 , validation loss: 6.536\n","Epoch: 1616 , train loss: 6.149 , validation loss: 6.608\n","Epoch: 1617 , train loss: 6.037 , validation loss: 6.552\n","Epoch: 1618 , train loss: 6.046 , validation loss: 6.562\n","Epoch: 1619 , train loss: 6.137 , validation loss: 6.540\n","Epoch: 1620 , train loss: 6.106 , validation loss: 6.661\n","Epoch: 1621 , train loss: 6.211 , validation loss: 6.600\n","Epoch: 1622 , train loss: 6.135 , validation loss: 6.634\n","Epoch: 1623 , train loss: 6.151 , validation loss: 6.541\n","Epoch: 1624 , train loss: 6.026 , validation loss: 6.564\n","Epoch: 1625 , train loss: 6.206 , validation loss: 6.603\n","Epoch: 1626 , train loss: 6.104 , validation loss: 6.548\n","Epoch: 1627 , train loss: 6.223 , validation loss: 6.575\n","Epoch: 1628 , train loss: 6.089 , validation loss: 6.565\n","Epoch: 1629 , train loss: 6.055 , validation loss: 6.633\n","Epoch: 1630 , train loss: 6.135 , validation loss: 6.569\n","Epoch: 1631 , train loss: 6.198 , validation loss: 6.619\n","Epoch: 1632 , train loss: 6.145 , validation loss: 6.554\n","Epoch: 1633 , train loss: 6.223 , validation loss: 6.565\n","Epoch: 1634 , train loss: 6.181 , validation loss: 6.590\n","Epoch: 1635 , train loss: 6.143 , validation loss: 6.590\n","Epoch: 1636 , train loss: 6.141 , validation loss: 6.547\n","Epoch: 1637 , train loss: 6.254 , validation loss: 6.522\n","Epoch: 1638 , train loss: 6.095 , validation loss: 6.608\n","Epoch: 1639 , train loss: 6.294 , validation loss: 6.488\n","Epoch: 1640 , train loss: 6.034 , validation loss: 6.546\n","Epoch: 1641 , train loss: 6.084 , validation loss: 6.599\n","Epoch: 1642 , train loss: 6.135 , validation loss: 6.556\n","Epoch: 1643 , train loss: 6.191 , validation loss: 6.647\n","Epoch: 1644 , train loss: 6.071 , validation loss: 6.588\n","Epoch: 1645 , train loss: 6.108 , validation loss: 6.551\n","Epoch: 1646 , train loss: 6.161 , validation loss: 6.599\n","Epoch: 1647 , train loss: 6.033 , validation loss: 6.610\n","Epoch: 1648 , train loss: 6.217 , validation loss: 6.641\n","Epoch: 1649 , train loss: 6.063 , validation loss: 6.537\n","Epoch: 1650 , train loss: 6.156 , validation loss: 6.544\n","Epoch: 1651 , train loss: 6.057 , validation loss: 6.596\n","Epoch: 1652 , train loss: 6.080 , validation loss: 6.629\n","Epoch: 1653 , train loss: 5.937 , validation loss: 6.532\n","Epoch: 1654 , train loss: 6.092 , validation loss: 6.591\n","Epoch: 1655 , train loss: 6.150 , validation loss: 6.561\n","Epoch: 1656 , train loss: 6.007 , validation loss: 6.576\n","Epoch: 1657 , train loss: 6.130 , validation loss: 6.540\n","Epoch: 1658 , train loss: 6.207 , validation loss: 6.558\n","Epoch: 1659 , train loss: 6.077 , validation loss: 6.569\n","Epoch: 1660 , train loss: 6.120 , validation loss: 6.565\n","Epoch: 1661 , train loss: 6.162 , validation loss: 6.482\n","Epoch: 1662 , train loss: 6.094 , validation loss: 6.474\n","Epoch: 1663 , train loss: 6.067 , validation loss: 6.479\n","Epoch: 1664 , train loss: 6.119 , validation loss: 6.516\n","Epoch: 1665 , train loss: 6.104 , validation loss: 6.533\n","Epoch: 1666 , train loss: 6.034 , validation loss: 6.441\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1667 , train loss: 6.126 , validation loss: 6.556\n","Epoch: 1668 , train loss: 6.130 , validation loss: 6.574\n","Epoch: 1669 , train loss: 6.158 , validation loss: 6.620\n","Epoch: 1670 , train loss: 6.255 , validation loss: 6.641\n","Epoch: 1671 , train loss: 6.120 , validation loss: 6.598\n","Epoch: 1672 , train loss: 6.106 , validation loss: 6.595\n","Epoch: 1673 , train loss: 6.017 , validation loss: 6.555\n","Epoch: 1674 , train loss: 6.059 , validation loss: 6.527\n","Epoch: 1675 , train loss: 6.057 , validation loss: 6.533\n","Epoch: 1676 , train loss: 6.144 , validation loss: 6.619\n","Epoch: 1677 , train loss: 6.170 , validation loss: 6.616\n","Epoch: 1678 , train loss: 6.177 , validation loss: 6.543\n","Epoch: 1679 , train loss: 6.156 , validation loss: 6.516\n","Epoch: 1680 , train loss: 6.110 , validation loss: 6.470\n","Epoch: 1681 , train loss: 6.002 , validation loss: 6.494\n","Epoch: 1682 , train loss: 6.078 , validation loss: 6.473\n","Epoch: 1683 , train loss: 6.111 , validation loss: 6.467\n","Epoch: 1684 , train loss: 6.145 , validation loss: 6.532\n","Epoch: 1685 , train loss: 6.196 , validation loss: 6.501\n","Epoch: 1686 , train loss: 5.989 , validation loss: 6.496\n","Epoch: 1687 , train loss: 5.993 , validation loss: 6.476\n","Epoch: 1688 , train loss: 6.131 , validation loss: 6.479\n","Epoch: 1689 , train loss: 6.161 , validation loss: 6.588\n","Epoch: 1690 , train loss: 6.031 , validation loss: 6.565\n","Epoch: 1691 , train loss: 6.085 , validation loss: 6.523\n","Epoch: 1692 , train loss: 5.969 , validation loss: 6.558\n","Epoch: 1693 , train loss: 6.017 , validation loss: 6.572\n","Epoch: 1694 , train loss: 6.132 , validation loss: 6.574\n","Epoch: 1695 , train loss: 6.106 , validation loss: 6.605\n","Epoch: 1696 , train loss: 6.137 , validation loss: 6.572\n","Epoch: 1697 , train loss: 6.028 , validation loss: 6.558\n","Epoch: 1698 , train loss: 6.090 , validation loss: 6.556\n","Epoch: 1699 , train loss: 6.169 , validation loss: 6.581\n","Epoch: 1700 , train loss: 6.075 , validation loss: 6.489\n","Epoch: 1701 , train loss: 5.904 , validation loss: 6.552\n","Epoch: 1702 , train loss: 6.124 , validation loss: 6.561\n","Epoch: 1703 , train loss: 6.179 , validation loss: 6.606\n","Epoch: 1704 , train loss: 6.248 , validation loss: 6.520\n","Epoch: 1705 , train loss: 6.027 , validation loss: 6.561\n","Epoch: 1706 , train loss: 5.956 , validation loss: 6.515\n","Epoch: 1707 , train loss: 6.102 , validation loss: 6.486\n","Epoch: 1708 , train loss: 6.169 , validation loss: 6.540\n","Epoch: 1709 , train loss: 6.169 , validation loss: 6.551\n","Epoch: 1710 , train loss: 6.175 , validation loss: 6.585\n","Epoch: 1711 , train loss: 6.122 , validation loss: 6.591\n","Epoch: 1712 , train loss: 6.136 , validation loss: 6.553\n","Epoch: 1713 , train loss: 6.160 , validation loss: 6.580\n","Epoch: 1714 , train loss: 6.281 , validation loss: 6.549\n","Epoch: 1715 , train loss: 6.101 , validation loss: 6.616\n","Epoch: 1716 , train loss: 6.134 , validation loss: 6.640\n","Epoch: 1717 , train loss: 6.131 , validation loss: 6.657\n","Epoch: 1718 , train loss: 5.998 , validation loss: 6.742\n","Epoch: 1719 , train loss: 6.109 , validation loss: 6.578\n","Epoch: 1720 , train loss: 6.078 , validation loss: 6.612\n","Epoch: 1721 , train loss: 6.079 , validation loss: 6.644\n","Epoch: 1722 , train loss: 6.157 , validation loss: 6.539\n","Epoch: 1723 , train loss: 6.082 , validation loss: 6.522\n","Epoch: 1724 , train loss: 6.076 , validation loss: 6.590\n","Epoch: 1725 , train loss: 6.097 , validation loss: 6.615\n","Epoch: 1726 , train loss: 5.976 , validation loss: 6.505\n","Epoch: 1727 , train loss: 6.087 , validation loss: 6.483\n","Epoch: 1728 , train loss: 6.149 , validation loss: 6.532\n","Epoch: 1729 , train loss: 6.156 , validation loss: 6.638\n","Epoch: 1730 , train loss: 6.006 , validation loss: 6.540\n","Epoch: 1731 , train loss: 6.071 , validation loss: 6.549\n","Epoch: 1732 , train loss: 6.081 , validation loss: 6.530\n","Epoch: 1733 , train loss: 6.177 , validation loss: 6.602\n","Epoch: 1734 , train loss: 5.998 , validation loss: 6.570\n","Epoch: 1735 , train loss: 6.035 , validation loss: 6.581\n","Epoch: 1736 , train loss: 5.969 , validation loss: 6.579\n","Epoch: 1737 , train loss: 6.096 , validation loss: 6.674\n","Epoch: 1738 , train loss: 6.072 , validation loss: 6.615\n","Epoch: 1739 , train loss: 6.157 , validation loss: 6.616\n","Epoch: 1740 , train loss: 6.088 , validation loss: 6.517\n","Epoch: 1741 , train loss: 6.070 , validation loss: 6.651\n","Epoch: 1742 , train loss: 6.075 , validation loss: 6.529\n","Epoch: 1743 , train loss: 6.134 , validation loss: 6.564\n","Epoch: 1744 , train loss: 6.194 , validation loss: 6.637\n","Epoch: 1745 , train loss: 6.086 , validation loss: 6.527\n","Epoch: 1746 , train loss: 6.083 , validation loss: 6.635\n","Epoch: 1747 , train loss: 6.241 , validation loss: 6.642\n","Epoch: 1748 , train loss: 6.090 , validation loss: 6.631\n","Epoch: 1749 , train loss: 6.138 , validation loss: 6.571\n","Epoch: 1750 , train loss: 6.043 , validation loss: 6.580\n","Epoch: 1751 , train loss: 6.017 , validation loss: 6.628\n","Epoch: 1752 , train loss: 5.969 , validation loss: 6.635\n","Epoch: 1753 , train loss: 5.986 , validation loss: 6.683\n","Epoch: 1754 , train loss: 6.084 , validation loss: 6.614\n","Epoch: 1755 , train loss: 5.931 , validation loss: 6.639\n","Epoch: 1756 , train loss: 6.034 , validation loss: 6.581\n","Epoch: 1757 , train loss: 5.963 , validation loss: 6.646\n","Epoch: 1758 , train loss: 6.122 , validation loss: 6.572\n","Epoch: 1759 , train loss: 6.085 , validation loss: 6.619\n","Epoch: 1760 , train loss: 6.040 , validation loss: 6.535\n","Epoch: 1761 , train loss: 6.016 , validation loss: 6.561\n","Epoch: 1762 , train loss: 6.048 , validation loss: 6.572\n","Epoch: 1763 , train loss: 6.000 , validation loss: 6.624\n","Epoch: 1764 , train loss: 6.166 , validation loss: 6.620\n","Epoch: 1765 , train loss: 6.060 , validation loss: 6.594\n","Epoch: 1766 , train loss: 6.076 , validation loss: 6.570\n","Epoch: 1767 , train loss: 6.167 , validation loss: 6.583\n","Epoch: 1768 , train loss: 6.019 , validation loss: 6.566\n","Epoch: 1769 , train loss: 6.156 , validation loss: 6.529\n","Epoch: 1770 , train loss: 6.088 , validation loss: 6.594\n","Epoch: 1771 , train loss: 6.111 , validation loss: 6.571\n","Epoch: 1772 , train loss: 6.079 , validation loss: 6.556\n","Epoch: 1773 , train loss: 6.026 , validation loss: 6.528\n","Epoch: 1774 , train loss: 6.166 , validation loss: 6.475\n","Epoch: 1775 , train loss: 5.952 , validation loss: 6.537\n","Epoch: 1776 , train loss: 6.082 , validation loss: 6.506\n","Epoch: 1777 , train loss: 6.104 , validation loss: 6.478\n","Epoch: 1778 , train loss: 6.163 , validation loss: 6.662\n","Epoch: 1779 , train loss: 6.088 , validation loss: 6.523\n","Epoch: 1780 , train loss: 6.016 , validation loss: 6.602\n","Epoch: 1781 , train loss: 6.192 , validation loss: 6.519\n","Epoch: 1782 , train loss: 6.025 , validation loss: 6.549\n","Epoch: 1783 , train loss: 6.096 , validation loss: 6.604\n","Epoch: 1784 , train loss: 6.000 , validation loss: 6.634\n","Epoch: 1785 , train loss: 6.051 , validation loss: 6.547\n","Epoch: 1786 , train loss: 6.183 , validation loss: 6.553\n","Epoch: 1787 , train loss: 5.939 , validation loss: 6.630\n","Epoch: 1788 , train loss: 5.974 , validation loss: 6.540\n","Epoch: 1789 , train loss: 6.182 , validation loss: 6.604\n","Epoch: 1790 , train loss: 6.102 , validation loss: 6.554\n","Epoch: 1791 , train loss: 6.049 , validation loss: 6.579\n","Epoch: 1792 , train loss: 6.073 , validation loss: 6.559\n","Epoch: 1793 , train loss: 6.026 , validation loss: 6.547\n","Epoch: 1794 , train loss: 6.129 , validation loss: 6.581\n","Epoch: 1795 , train loss: 6.059 , validation loss: 6.604\n","Epoch: 1796 , train loss: 6.083 , validation loss: 6.468\n","Epoch: 1797 , train loss: 6.044 , validation loss: 6.579\n","Epoch: 1798 , train loss: 5.994 , validation loss: 6.583\n","Epoch: 1799 , train loss: 6.154 , validation loss: 6.419\n","Model saved in path: ./logs/LastVersion/LastVersion.ckpt\n","Epoch: 1800 , train loss: 5.936 , validation loss: 6.535\n","Epoch: 1801 , train loss: 5.933 , validation loss: 6.506\n","Epoch: 1802 , train loss: 5.968 , validation loss: 6.489\n","Epoch: 1803 , train loss: 6.101 , validation loss: 6.482\n","Epoch: 1804 , train loss: 6.263 , validation loss: 6.615\n","Epoch: 1805 , train loss: 6.148 , validation loss: 6.500\n","Epoch: 1806 , train loss: 6.057 , validation loss: 6.524\n","Epoch: 1807 , train loss: 6.028 , validation loss: 6.539\n","Epoch: 1808 , train loss: 6.125 , validation loss: 6.564\n","Epoch: 1809 , train loss: 6.094 , validation loss: 6.508\n","Epoch: 1810 , train loss: 6.009 , validation loss: 6.553\n","Epoch: 1811 , train loss: 5.992 , validation loss: 6.498\n","Epoch: 1812 , train loss: 5.899 , validation loss: 6.495\n","Epoch: 1813 , train loss: 6.069 , validation loss: 6.586\n","Epoch: 1814 , train loss: 6.118 , validation loss: 6.528\n","Epoch: 1815 , train loss: 6.124 , validation loss: 6.526\n","Epoch: 1816 , train loss: 6.068 , validation loss: 6.551\n","Epoch: 1817 , train loss: 6.035 , validation loss: 6.561\n","Epoch: 1818 , train loss: 6.076 , validation loss: 6.588\n","Epoch: 1819 , train loss: 6.004 , validation loss: 6.590\n","Epoch: 1820 , train loss: 6.068 , validation loss: 6.584\n","Epoch: 1821 , train loss: 6.115 , validation loss: 6.671\n","Epoch: 1822 , train loss: 6.036 , validation loss: 6.528\n","Epoch: 1823 , train loss: 5.980 , validation loss: 6.467\n","Epoch: 1824 , train loss: 6.154 , validation loss: 6.591\n","Epoch: 1825 , train loss: 6.024 , validation loss: 6.543\n","Epoch: 1826 , train loss: 6.143 , validation loss: 6.582\n","Epoch: 1827 , train loss: 5.924 , validation loss: 6.603\n","Epoch: 1828 , train loss: 6.080 , validation loss: 6.593\n","Epoch: 1829 , train loss: 6.032 , validation loss: 6.573\n","Epoch: 1830 , train loss: 6.031 , validation loss: 6.633\n","Epoch: 1831 , train loss: 6.099 , validation loss: 6.539\n","Epoch: 1832 , train loss: 6.114 , validation loss: 6.483\n","Epoch: 1833 , train loss: 6.080 , validation loss: 6.558\n","Epoch: 1834 , train loss: 6.165 , validation loss: 6.592\n","Epoch: 1835 , train loss: 5.885 , validation loss: 6.512\n","Epoch: 1836 , train loss: 6.164 , validation loss: 6.542\n","Epoch: 1837 , train loss: 5.994 , validation loss: 6.569\n","Epoch: 1838 , train loss: 6.064 , validation loss: 6.606\n","Epoch: 1839 , train loss: 6.015 , validation loss: 6.604\n","Epoch: 1840 , train loss: 6.141 , validation loss: 6.553\n","Epoch: 1841 , train loss: 5.983 , validation loss: 6.590\n","Epoch: 1842 , train loss: 6.009 , validation loss: 6.606\n","Epoch: 1843 , train loss: 6.012 , validation loss: 6.616\n","Epoch: 1844 , train loss: 6.000 , validation loss: 6.716\n","Epoch: 1845 , train loss: 5.999 , validation loss: 6.544\n","Epoch: 1846 , train loss: 6.195 , validation loss: 6.584\n","Epoch: 1847 , train loss: 6.056 , validation loss: 6.551\n","Epoch: 1848 , train loss: 5.923 , validation loss: 6.619\n","Epoch: 1849 , train loss: 6.047 , validation loss: 6.599\n","Epoch: 1850 , train loss: 6.057 , validation loss: 6.670\n","Epoch: 1851 , train loss: 6.051 , validation loss: 6.612\n","Epoch: 1852 , train loss: 6.071 , validation loss: 6.527\n","Epoch: 1853 , train loss: 6.137 , validation loss: 6.551\n","Epoch: 1854 , train loss: 6.084 , validation loss: 6.499\n","Epoch: 1855 , train loss: 6.127 , validation loss: 6.540\n","Epoch: 1856 , train loss: 6.063 , validation loss: 6.650\n","Epoch: 1857 , train loss: 6.052 , validation loss: 6.621\n","Epoch: 1858 , train loss: 6.221 , validation loss: 6.541\n","Epoch: 1859 , train loss: 5.964 , validation loss: 6.528\n","Epoch: 1860 , train loss: 5.923 , validation loss: 6.540\n","Epoch: 1861 , train loss: 5.951 , validation loss: 6.513\n","Epoch: 1862 , train loss: 6.083 , validation loss: 6.598\n","Epoch: 1863 , train loss: 6.029 , validation loss: 6.556\n","Epoch: 1864 , train loss: 5.979 , validation loss: 6.497\n","Epoch: 1865 , train loss: 5.992 , validation loss: 6.511\n","Epoch: 1866 , train loss: 5.999 , validation loss: 6.581\n","Epoch: 1867 , train loss: 6.029 , validation loss: 6.526\n","Epoch: 1868 , train loss: 6.030 , validation loss: 6.579\n","Epoch: 1869 , train loss: 5.990 , validation loss: 6.545\n","Epoch: 1870 , train loss: 6.047 , validation loss: 6.526\n","Epoch: 1871 , train loss: 5.906 , validation loss: 6.540\n","Epoch: 1872 , train loss: 5.983 , validation loss: 6.535\n","Epoch: 1873 , train loss: 6.008 , validation loss: 6.473\n","Epoch: 1874 , train loss: 5.945 , validation loss: 6.480\n","Epoch: 1875 , train loss: 5.987 , validation loss: 6.521\n","Epoch: 1876 , train loss: 6.013 , validation loss: 6.524\n","Epoch: 1877 , train loss: 5.954 , validation loss: 6.502\n","Epoch: 1878 , train loss: 6.054 , validation loss: 6.444\n","Epoch: 1879 , train loss: 6.046 , validation loss: 6.579\n","Epoch: 1880 , train loss: 5.974 , validation loss: 6.528\n","Epoch: 1881 , train loss: 5.999 , validation loss: 6.515\n","Epoch: 1882 , train loss: 6.047 , validation loss: 6.471\n","Epoch: 1883 , train loss: 5.871 , validation loss: 6.573\n","Epoch: 1884 , train loss: 6.005 , validation loss: 6.565\n","Epoch: 1885 , train loss: 6.136 , validation loss: 6.604\n","Epoch: 1886 , train loss: 5.938 , validation loss: 6.533\n","Epoch: 1887 , train loss: 6.000 , validation loss: 6.536\n","Epoch: 1888 , train loss: 6.022 , validation loss: 6.535\n","Epoch: 1889 , train loss: 6.018 , validation loss: 6.533\n","Epoch: 1890 , train loss: 5.922 , validation loss: 6.504\n","Epoch: 1891 , train loss: 5.929 , validation loss: 6.523\n","Epoch: 1892 , train loss: 5.960 , validation loss: 6.460\n","Epoch: 1893 , train loss: 6.025 , validation loss: 6.577\n","Epoch: 1894 , train loss: 6.084 , validation loss: 6.463\n","Epoch: 1895 , train loss: 5.909 , validation loss: 6.533\n","Epoch: 1896 , train loss: 5.978 , validation loss: 6.549\n","Epoch: 1897 , train loss: 5.805 , validation loss: 6.515\n","Epoch: 1898 , train loss: 6.039 , validation loss: 6.543\n","Epoch: 1899 , train loss: 5.994 , validation loss: 6.540\n","Epoch: 1900 , train loss: 6.011 , validation loss: 6.589\n","Epoch: 1901 , train loss: 5.945 , validation loss: 6.602\n","Epoch: 1902 , train loss: 5.986 , validation loss: 6.567\n","Epoch: 1903 , train loss: 6.014 , validation loss: 6.568\n","Epoch: 1904 , train loss: 6.027 , validation loss: 6.568\n","Epoch: 1905 , train loss: 6.046 , validation loss: 6.608\n","Epoch: 1906 , train loss: 5.949 , validation loss: 6.518\n","Epoch: 1907 , train loss: 6.052 , validation loss: 6.495\n","Epoch: 1908 , train loss: 6.013 , validation loss: 6.537\n","Epoch: 1909 , train loss: 5.981 , validation loss: 6.621\n","Epoch: 1910 , train loss: 6.041 , validation loss: 6.505\n","Epoch: 1911 , train loss: 6.032 , validation loss: 6.517\n","Epoch: 1912 , train loss: 6.105 , validation loss: 6.568\n","Epoch: 1913 , train loss: 5.789 , validation loss: 6.541\n","Epoch: 1914 , train loss: 5.951 , validation loss: 6.571\n","Epoch: 1915 , train loss: 6.145 , validation loss: 6.488\n","Epoch: 1916 , train loss: 6.008 , validation loss: 6.510\n","Epoch: 1917 , train loss: 5.972 , validation loss: 6.549\n","Epoch: 1918 , train loss: 5.878 , validation loss: 6.513\n","Epoch: 1919 , train loss: 6.010 , validation loss: 6.532\n","Epoch: 1920 , train loss: 5.971 , validation loss: 6.506\n","Epoch: 1921 , train loss: 5.916 , validation loss: 6.516\n","Epoch: 1922 , train loss: 5.933 , validation loss: 6.594\n","Epoch: 1923 , train loss: 5.964 , validation loss: 6.546\n","Epoch: 1924 , train loss: 6.025 , validation loss: 6.592\n","Epoch: 1925 , train loss: 6.047 , validation loss: 6.581\n","Epoch: 1926 , train loss: 5.979 , validation loss: 6.562\n","Epoch: 1927 , train loss: 5.881 , validation loss: 6.576\n","Epoch: 1928 , train loss: 5.902 , validation loss: 6.587\n","Epoch: 1929 , train loss: 5.893 , validation loss: 6.571\n","Epoch: 1930 , train loss: 6.097 , validation loss: 6.571\n","Epoch: 1931 , train loss: 6.042 , validation loss: 6.518\n","Epoch: 1932 , train loss: 5.928 , validation loss: 6.574\n","Epoch: 1933 , train loss: 5.972 , validation loss: 6.568\n","Epoch: 1934 , train loss: 6.027 , validation loss: 6.569\n","Epoch: 1935 , train loss: 5.915 , validation loss: 6.562\n","Epoch: 1936 , train loss: 6.037 , validation loss: 6.524\n","Epoch: 1937 , train loss: 5.941 , validation loss: 6.516\n","Epoch: 1938 , train loss: 6.209 , validation loss: 6.484\n","Epoch: 1939 , train loss: 5.991 , validation loss: 6.597\n","Epoch: 1940 , train loss: 5.941 , validation loss: 6.584\n","Epoch: 1941 , train loss: 5.986 , validation loss: 6.560\n","Epoch: 1942 , train loss: 5.956 , validation loss: 6.609\n","Epoch: 1943 , train loss: 5.901 , validation loss: 6.578\n","Epoch: 1944 , train loss: 5.941 , validation loss: 6.491\n","Epoch: 1945 , train loss: 5.961 , validation loss: 6.565\n","Epoch: 1946 , train loss: 5.789 , validation loss: 6.653\n","Epoch: 1947 , train loss: 5.988 , validation loss: 6.556\n","Epoch: 1948 , train loss: 6.041 , validation loss: 6.579\n","Epoch: 1949 , train loss: 6.034 , validation loss: 6.624\n","Epoch: 1950 , train loss: 5.910 , validation loss: 6.561\n","Epoch: 1951 , train loss: 5.998 , validation loss: 6.596\n","Epoch: 1952 , train loss: 5.923 , validation loss: 6.535\n","Epoch: 1953 , train loss: 6.046 , validation loss: 6.632\n","Epoch: 1954 , train loss: 6.017 , validation loss: 6.537\n","Epoch: 1955 , train loss: 6.006 , validation loss: 6.616\n","Epoch: 1956 , train loss: 5.858 , validation loss: 6.576\n","Epoch: 1957 , train loss: 5.965 , validation loss: 6.594\n","Epoch: 1958 , train loss: 5.898 , validation loss: 6.510\n","Epoch: 1959 , train loss: 6.152 , validation loss: 6.615\n","Epoch: 1960 , train loss: 5.992 , validation loss: 6.590\n","Epoch: 1961 , train loss: 6.002 , validation loss: 6.550\n","Epoch: 1962 , train loss: 6.043 , validation loss: 6.542\n","Epoch: 1963 , train loss: 5.984 , validation loss: 6.561\n","Epoch: 1964 , train loss: 5.986 , validation loss: 6.567\n","Epoch: 1965 , train loss: 5.927 , validation loss: 6.503\n","Epoch: 1966 , train loss: 5.982 , validation loss: 6.552\n","Epoch: 1967 , train loss: 5.940 , validation loss: 6.495\n","Epoch: 1968 , train loss: 5.944 , validation loss: 6.621\n","Epoch: 1969 , train loss: 5.998 , validation loss: 6.603\n","Epoch: 1970 , train loss: 5.969 , validation loss: 6.562\n","Epoch: 1971 , train loss: 5.995 , validation loss: 6.570\n","Epoch: 1972 , train loss: 6.034 , validation loss: 6.578\n","Epoch: 1973 , train loss: 5.874 , validation loss: 6.562\n","Epoch: 1974 , train loss: 5.892 , validation loss: 6.547\n","Epoch: 1975 , train loss: 5.894 , validation loss: 6.598\n","Epoch: 1976 , train loss: 6.003 , validation loss: 6.631\n","Epoch: 1977 , train loss: 6.061 , validation loss: 6.597\n","Epoch: 1978 , train loss: 5.959 , validation loss: 6.593\n","Epoch: 1979 , train loss: 5.958 , validation loss: 6.637\n","Epoch: 1980 , train loss: 6.091 , validation loss: 6.575\n","Epoch: 1981 , train loss: 5.906 , validation loss: 6.629\n","Epoch: 1982 , train loss: 5.890 , validation loss: 6.546\n","Epoch: 1983 , train loss: 5.897 , validation loss: 6.524\n","Epoch: 1984 , train loss: 5.892 , validation loss: 6.561\n","Epoch: 1985 , train loss: 5.988 , validation loss: 6.670\n","Epoch: 1986 , train loss: 5.887 , validation loss: 6.599\n","Epoch: 1987 , train loss: 5.878 , validation loss: 6.585\n","Epoch: 1988 , train loss: 5.869 , validation loss: 6.530\n","Epoch: 1989 , train loss: 6.015 , validation loss: 6.564\n","Epoch: 1990 , train loss: 5.955 , validation loss: 6.625\n","Epoch: 1991 , train loss: 5.887 , validation loss: 6.537\n","Epoch: 1992 , train loss: 5.866 , validation loss: 6.562\n","Epoch: 1993 , train loss: 6.007 , validation loss: 6.591\n","Epoch: 1994 , train loss: 5.917 , validation loss: 6.569\n","Epoch: 1995 , train loss: 5.872 , validation loss: 6.557\n","Epoch: 1996 , train loss: 5.933 , validation loss: 6.484\n","Epoch: 1997 , train loss: 6.082 , validation loss: 6.542\n","Epoch: 1998 , train loss: 5.951 , validation loss: 6.547\n","Epoch: 1999 , train loss: 5.913 , validation loss: 6.607\n","Epoch: 2000 , train loss: 5.971 , validation loss: 6.582\n","Epoch: 2001 , train loss: 5.997 , validation loss: 6.659\n","Epoch: 2002 , train loss: 5.947 , validation loss: 6.651\n","Epoch: 2003 , train loss: 5.924 , validation loss: 6.655\n","Epoch: 2004 , train loss: 5.955 , validation loss: 6.576\n","Epoch: 2005 , train loss: 6.044 , validation loss: 6.648\n","Epoch: 2006 , train loss: 5.913 , validation loss: 6.591\n","Epoch: 2007 , train loss: 5.777 , validation loss: 6.556\n","Epoch: 2008 , train loss: 6.116 , validation loss: 6.572\n","Epoch: 2009 , train loss: 6.008 , validation loss: 6.555\n","Epoch: 2010 , train loss: 5.952 , validation loss: 6.547\n","Epoch: 2011 , train loss: 5.936 , validation loss: 6.594\n","Epoch: 2012 , train loss: 5.963 , validation loss: 6.655\n","Epoch: 2013 , train loss: 5.951 , validation loss: 6.505\n","Epoch: 2014 , train loss: 5.938 , validation loss: 6.584\n","Epoch: 2015 , train loss: 5.963 , validation loss: 6.649\n","Epoch: 2016 , train loss: 5.947 , validation loss: 6.542\n","Epoch: 2017 , train loss: 5.955 , validation loss: 6.550\n","Epoch: 2018 , train loss: 6.077 , validation loss: 6.595\n","Epoch: 2019 , train loss: 5.929 , validation loss: 6.614\n","Epoch: 2020 , train loss: 5.864 , validation loss: 6.661\n","Epoch: 2021 , train loss: 5.927 , validation loss: 6.585\n","Epoch: 2022 , train loss: 6.050 , validation loss: 6.663\n","Epoch: 2023 , train loss: 5.930 , validation loss: 6.610\n","Epoch: 2024 , train loss: 5.987 , validation loss: 6.505\n","Epoch: 2025 , train loss: 5.782 , validation loss: 6.583\n","Epoch: 2026 , train loss: 5.831 , validation loss: 6.560\n","Epoch: 2027 , train loss: 5.988 , validation loss: 6.572\n","Epoch: 2028 , train loss: 6.007 , validation loss: 6.721\n","Epoch: 2029 , train loss: 6.020 , validation loss: 6.697\n","Epoch: 2030 , train loss: 5.865 , validation loss: 6.670\n","Epoch: 2031 , train loss: 5.965 , validation loss: 6.669\n","Epoch: 2032 , train loss: 5.875 , validation loss: 6.571\n","Epoch: 2033 , train loss: 5.969 , validation loss: 6.627\n","Epoch: 2034 , train loss: 5.953 , validation loss: 6.621\n","Epoch: 2035 , train loss: 5.979 , validation loss: 6.587\n","Epoch: 2036 , train loss: 5.887 , validation loss: 6.582\n","Epoch: 2037 , train loss: 6.012 , validation loss: 6.574\n","Epoch: 2038 , train loss: 5.914 , validation loss: 6.513\n","Epoch: 2039 , train loss: 5.962 , validation loss: 6.522\n","Epoch: 2040 , train loss: 5.811 , validation loss: 6.692\n","Epoch: 2041 , train loss: 6.007 , validation loss: 6.572\n","Epoch: 2042 , train loss: 5.868 , validation loss: 6.692\n","Epoch: 2043 , train loss: 5.935 , validation loss: 6.610\n","Epoch: 2044 , train loss: 6.059 , validation loss: 6.545\n","Epoch: 2045 , train loss: 5.924 , validation loss: 6.473\n","Epoch: 2046 , train loss: 5.838 , validation loss: 6.553\n","Epoch: 2047 , train loss: 5.998 , validation loss: 6.622\n","Epoch: 2048 , train loss: 5.887 , validation loss: 6.555\n","Epoch: 2049 , train loss: 5.898 , validation loss: 6.619\n","Epoch: 2050 , train loss: 5.932 , validation loss: 6.498\n","Epoch: 2051 , train loss: 5.945 , validation loss: 6.534\n","Epoch: 2052 , train loss: 5.894 , validation loss: 6.453\n","Epoch: 2053 , train loss: 5.796 , validation loss: 6.558\n","Epoch: 2054 , train loss: 5.937 , validation loss: 6.516\n","Epoch: 2055 , train loss: 5.841 , validation loss: 6.576\n","Epoch: 2056 , train loss: 5.933 , validation loss: 6.537\n","Epoch: 2057 , train loss: 5.986 , validation loss: 6.548\n","Epoch: 2058 , train loss: 5.914 , validation loss: 6.540\n","Epoch: 2059 , train loss: 5.884 , validation loss: 6.674\n","Epoch: 2060 , train loss: 5.955 , validation loss: 6.570\n","Epoch: 2061 , train loss: 5.850 , validation loss: 6.615\n","Epoch: 2062 , train loss: 5.872 , validation loss: 6.565\n","Epoch: 2063 , train loss: 5.869 , validation loss: 6.619\n","Epoch: 2064 , train loss: 5.944 , validation loss: 6.576\n","Epoch: 2065 , train loss: 5.904 , validation loss: 6.565\n","Epoch: 2066 , train loss: 5.952 , validation loss: 6.648\n","Epoch: 2067 , train loss: 5.882 , validation loss: 6.635\n","Epoch: 2068 , train loss: 5.930 , validation loss: 6.575\n","Epoch: 2069 , train loss: 5.814 , validation loss: 6.616\n","Epoch: 2070 , train loss: 5.839 , validation loss: 6.629\n","Epoch: 2071 , train loss: 6.013 , validation loss: 6.613\n","Epoch: 2072 , train loss: 5.988 , validation loss: 6.660\n","Epoch: 2073 , train loss: 5.916 , validation loss: 6.644\n","Epoch: 2074 , train loss: 5.922 , validation loss: 6.657\n","Epoch: 2075 , train loss: 5.922 , validation loss: 6.609\n","Epoch: 2076 , train loss: 5.811 , validation loss: 6.675\n","Epoch: 2077 , train loss: 5.964 , validation loss: 6.594\n","Epoch: 2078 , train loss: 5.823 , validation loss: 6.637\n","Epoch: 2079 , train loss: 5.889 , validation loss: 6.618\n","Epoch: 2080 , train loss: 5.868 , validation loss: 6.566\n","Epoch: 2081 , train loss: 5.949 , validation loss: 6.541\n","Epoch: 2082 , train loss: 5.846 , validation loss: 9.146\n","Epoch: 2083 , train loss: 5.926 , validation loss: 6.601\n","Epoch: 2084 , train loss: 5.866 , validation loss: 6.527\n","Epoch: 2085 , train loss: 5.899 , validation loss: 6.590\n","Epoch: 2086 , train loss: 5.921 , validation loss: 6.680\n","Epoch: 2087 , train loss: 5.896 , validation loss: 6.587\n","Epoch: 2088 , train loss: 5.936 , validation loss: 6.557\n","Epoch: 2089 , train loss: 5.950 , validation loss: 6.518\n","Epoch: 2090 , train loss: 5.859 , validation loss: 6.528\n","Epoch: 2091 , train loss: 5.814 , validation loss: 6.538\n","Epoch: 2092 , train loss: 5.811 , validation loss: 6.627\n","Epoch: 2093 , train loss: 5.852 , validation loss: 6.583\n","Epoch: 2094 , train loss: 6.003 , validation loss: 6.583\n","Epoch: 2095 , train loss: 5.868 , validation loss: 6.495\n","Epoch: 2096 , train loss: 5.790 , validation loss: 6.537\n","Epoch: 2097 , train loss: 5.893 , validation loss: 6.541\n","Epoch: 2098 , train loss: 5.903 , validation loss: 6.617\n","Epoch: 2099 , train loss: 6.056 , validation loss: 6.612\n","Epoch: 2100 , train loss: 5.899 , validation loss: 6.590\n","Epoch: 2101 , train loss: 5.943 , validation loss: 6.564\n","Epoch: 2102 , train loss: 5.899 , validation loss: 6.534\n","Epoch: 2103 , train loss: 5.869 , validation loss: 6.554\n","Epoch: 2104 , train loss: 5.936 , validation loss: 6.585\n","Epoch: 2105 , train loss: 5.835 , validation loss: 6.537\n","Epoch: 2106 , train loss: 6.056 , validation loss: 6.525\n","Epoch: 2107 , train loss: 5.817 , validation loss: 6.519\n","Epoch: 2108 , train loss: 5.961 , validation loss: 6.538\n","Epoch: 2109 , train loss: 5.902 , validation loss: 6.551\n","Epoch: 2110 , train loss: 5.960 , validation loss: 6.521\n","Epoch: 2111 , train loss: 5.851 , validation loss: 6.562\n","Epoch: 2112 , train loss: 5.857 , validation loss: 6.586\n","Epoch: 2113 , train loss: 5.859 , validation loss: 6.521\n","Epoch: 2114 , train loss: 5.911 , validation loss: 6.537\n","Epoch: 2115 , train loss: 5.911 , validation loss: 6.617\n","Epoch: 2116 , train loss: 5.858 , validation loss: 6.552\n","Epoch: 2117 , train loss: 5.982 , validation loss: 6.536\n","Epoch: 2118 , train loss: 5.940 , validation loss: 6.510\n","Epoch: 2119 , train loss: 5.879 , validation loss: 6.685\n","Epoch: 2120 , train loss: 5.863 , validation loss: 6.610\n","Epoch: 2121 , train loss: 5.891 , validation loss: 6.545\n","Epoch: 2122 , train loss: 5.879 , validation loss: 6.565\n","Epoch: 2123 , train loss: 6.037 , validation loss: 6.524\n","Epoch: 2124 , train loss: 5.937 , validation loss: 6.505\n","Epoch: 2125 , train loss: 6.050 , validation loss: 6.518\n","Epoch: 2126 , train loss: 5.876 , validation loss: 6.503\n","Epoch: 2127 , train loss: 5.894 , validation loss: 6.520\n","Epoch: 2128 , train loss: 5.928 , validation loss: 6.503\n","Epoch: 2129 , train loss: 5.860 , validation loss: 6.614\n","Epoch: 2130 , train loss: 5.964 , validation loss: 6.528\n","Epoch: 2131 , train loss: 5.831 , validation loss: 6.509\n","Epoch: 2132 , train loss: 5.871 , validation loss: 6.542\n","Epoch: 2133 , train loss: 5.905 , validation loss: 6.479\n","Epoch: 2134 , train loss: 5.990 , validation loss: 6.492\n","Epoch: 2135 , train loss: 5.924 , validation loss: 6.463\n","Epoch: 2136 , train loss: 5.884 , validation loss: 6.559\n","Epoch: 2137 , train loss: 5.841 , validation loss: 6.486\n","Epoch: 2138 , train loss: 5.799 , validation loss: 6.578\n","Epoch: 2139 , train loss: 5.985 , validation loss: 6.568\n","Epoch: 2140 , train loss: 5.788 , validation loss: 6.556\n","Epoch: 2141 , train loss: 5.930 , validation loss: 6.499\n","Epoch: 2142 , train loss: 5.910 , validation loss: 6.534\n","Epoch: 2143 , train loss: 5.898 , validation loss: 6.497\n","Epoch: 2144 , train loss: 5.950 , validation loss: 6.625\n","Epoch: 2145 , train loss: 5.831 , validation loss: 6.603\n","Epoch: 2146 , train loss: 5.896 , validation loss: 6.590\n","Epoch: 2147 , train loss: 5.836 , validation loss: 6.538\n","Epoch: 2148 , train loss: 5.946 , validation loss: 6.590\n","Epoch: 2149 , train loss: 6.051 , validation loss: 6.580\n","Epoch: 2150 , train loss: 5.910 , validation loss: 6.598\n","Epoch: 2151 , train loss: 5.903 , validation loss: 6.546\n","Epoch: 2152 , train loss: 5.789 , validation loss: 6.596\n","Epoch: 2153 , train loss: 5.783 , validation loss: 6.587\n","Epoch: 2154 , train loss: 5.924 , validation loss: 6.612\n","Epoch: 2155 , train loss: 5.908 , validation loss: 6.549\n","Epoch: 2156 , train loss: 5.971 , validation loss: 6.562\n","Epoch: 2157 , train loss: 5.731 , validation loss: 6.572\n","Epoch: 2158 , train loss: 5.861 , validation loss: 6.545\n","Epoch: 2159 , train loss: 5.764 , validation loss: 6.479\n","Epoch: 2160 , train loss: 5.954 , validation loss: 6.533\n","Epoch: 2161 , train loss: 5.747 , validation loss: 6.640\n","Epoch: 2162 , train loss: 5.977 , validation loss: 6.619\n","Epoch: 2163 , train loss: 5.918 , validation loss: 6.522\n","Epoch: 2164 , train loss: 5.827 , validation loss: 6.578\n","Epoch: 2165 , train loss: 5.764 , validation loss: 6.569\n","Epoch: 2166 , train loss: 5.929 , validation loss: 6.532\n","Epoch: 2167 , train loss: 5.824 , validation loss: 6.534\n","Epoch: 2168 , train loss: 5.821 , validation loss: 6.565\n","Epoch: 2169 , train loss: 5.832 , validation loss: 6.518\n","Epoch: 2170 , train loss: 5.831 , validation loss: 6.548\n","Epoch: 2171 , train loss: 5.949 , validation loss: 6.582\n","Epoch: 2172 , train loss: 5.812 , validation loss: 6.643\n","Epoch: 2173 , train loss: 5.872 , validation loss: 6.597\n","Epoch: 2174 , train loss: 5.909 , validation loss: 6.607\n","Epoch: 2175 , train loss: 5.853 , validation loss: 6.570\n","Epoch: 2176 , train loss: 5.825 , validation loss: 6.568\n","Epoch: 2177 , train loss: 5.927 , validation loss: 6.523\n","Epoch: 2178 , train loss: 5.878 , validation loss: 6.498\n","Epoch: 2179 , train loss: 6.028 , validation loss: 6.508\n","Epoch: 2180 , train loss: 6.048 , validation loss: 6.507\n","Epoch: 2181 , train loss: 5.801 , validation loss: 6.565\n","Epoch: 2182 , train loss: 5.732 , validation loss: 6.478\n","Epoch: 2183 , train loss: 5.918 , validation loss: 6.476\n","Epoch: 2184 , train loss: 5.920 , validation loss: 6.499\n","Epoch: 2185 , train loss: 5.793 , validation loss: 6.519\n","Epoch: 2186 , train loss: 5.868 , validation loss: 6.526\n","Epoch: 2187 , train loss: 6.007 , validation loss: 6.523\n","Epoch: 2188 , train loss: 5.771 , validation loss: 6.532\n","Epoch: 2189 , train loss: 5.944 , validation loss: 6.544\n","Epoch: 2190 , train loss: 5.763 , validation loss: 6.580\n","Epoch: 2191 , train loss: 5.954 , validation loss: 6.594\n","Epoch: 2192 , train loss: 5.828 , validation loss: 6.655\n","Epoch: 2193 , train loss: 5.835 , validation loss: 6.553\n","Epoch: 2194 , train loss: 5.860 , validation loss: 6.538\n","Epoch: 2195 , train loss: 5.826 , validation loss: 6.528\n","Epoch: 2196 , train loss: 5.879 , validation loss: 6.576\n","Epoch: 2197 , train loss: 5.839 , validation loss: 6.563\n","Epoch: 2198 , train loss: 5.868 , validation loss: 6.542\n","Epoch: 2199 , train loss: 5.896 , validation loss: 6.562\n","Epoch: 2200 , train loss: 5.884 , validation loss: 6.496\n","Epoch: 2201 , train loss: 5.854 , validation loss: 6.573\n","Epoch: 2202 , train loss: 5.860 , validation loss: 6.519\n","Epoch: 2203 , train loss: 5.792 , validation loss: 6.507\n","Epoch: 2204 , train loss: 5.849 , validation loss: 6.478\n","Epoch: 2205 , train loss: 5.853 , validation loss: 6.578\n","Epoch: 2206 , train loss: 5.853 , validation loss: 6.517\n","Epoch: 2207 , train loss: 5.916 , validation loss: 6.537\n","Epoch: 2208 , train loss: 5.840 , validation loss: 6.496\n","Epoch: 2209 , train loss: 6.011 , validation loss: 6.480\n","Epoch: 2210 , train loss: 5.793 , validation loss: 6.460\n","Epoch: 2211 , train loss: 5.824 , validation loss: 6.432\n","Epoch: 2212 , train loss: 6.026 , validation loss: 6.613\n","Epoch: 2213 , train loss: 5.864 , validation loss: 6.533\n","Epoch: 2214 , train loss: 5.685 , validation loss: 6.460\n","Epoch: 2215 , train loss: 5.890 , validation loss: 6.522\n","Epoch: 2216 , train loss: 5.921 , validation loss: 6.536\n","Epoch: 2217 , train loss: 5.835 , validation loss: 6.509\n","Epoch: 2218 , train loss: 5.929 , validation loss: 6.495\n","Epoch: 2219 , train loss: 5.872 , validation loss: 6.506\n","Epoch: 2220 , train loss: 5.790 , validation loss: 6.515\n","Epoch: 2221 , train loss: 5.866 , validation loss: 6.514\n","Epoch: 2222 , train loss: 5.854 , validation loss: 6.521\n","Epoch: 2223 , train loss: 5.884 , validation loss: 6.519\n","Epoch: 2224 , train loss: 5.771 , validation loss: 6.473\n","Epoch: 2225 , train loss: 5.780 , validation loss: 6.461\n","Epoch: 2226 , train loss: 5.934 , validation loss: 6.571\n","Epoch: 2227 , train loss: 5.848 , validation loss: 6.480\n","Epoch: 2228 , train loss: 5.830 , validation loss: 6.575\n","Epoch: 2229 , train loss: 5.804 , validation loss: 6.512\n","Epoch: 2230 , train loss: 5.745 , validation loss: 6.598\n","Epoch: 2231 , train loss: 5.708 , validation loss: 6.563\n","Epoch: 2232 , train loss: 5.986 , validation loss: 6.582\n","Epoch: 2233 , train loss: 5.812 , validation loss: 6.530\n","Epoch: 2234 , train loss: 5.884 , validation loss: 6.565\n","Epoch: 2235 , train loss: 5.819 , validation loss: 6.578\n","Epoch: 2236 , train loss: 5.919 , validation loss: 6.664\n","Epoch: 2237 , train loss: 5.716 , validation loss: 6.581\n","Epoch: 2238 , train loss: 5.768 , validation loss: 6.590\n","Epoch: 2239 , train loss: 5.847 , validation loss: 6.546\n","Epoch: 2240 , train loss: 5.766 , validation loss: 6.595\n","Epoch: 2241 , train loss: 5.980 , validation loss: 6.530\n","Epoch: 2242 , train loss: 5.850 , validation loss: 6.506\n","Epoch: 2243 , train loss: 5.819 , validation loss: 6.520\n","Epoch: 2244 , train loss: 5.851 , validation loss: 6.502\n","Epoch: 2245 , train loss: 5.819 , validation loss: 6.604\n","Epoch: 2246 , train loss: 5.783 , validation loss: 6.559\n","Epoch: 2247 , train loss: 5.863 , validation loss: 6.552\n","Epoch: 2248 , train loss: 5.889 , validation loss: 6.624\n","Epoch: 2249 , train loss: 5.676 , validation loss: 6.559\n","Epoch: 2250 , train loss: 5.916 , validation loss: 6.550\n","Epoch: 2251 , train loss: 5.833 , validation loss: 6.522\n","Epoch: 2252 , train loss: 5.981 , validation loss: 6.579\n","Epoch: 2253 , train loss: 5.749 , validation loss: 6.556\n","Epoch: 2254 , train loss: 5.887 , validation loss: 6.534\n","Epoch: 2255 , train loss: 5.845 , validation loss: 6.522\n","Epoch: 2256 , train loss: 5.859 , validation loss: 6.545\n","Epoch: 2257 , train loss: 5.704 , validation loss: 6.594\n","Epoch: 2258 , train loss: 5.811 , validation loss: 6.667\n","Epoch: 2259 , train loss: 5.885 , validation loss: 6.541\n","Epoch: 2260 , train loss: 5.952 , validation loss: 6.547\n","Epoch: 2261 , train loss: 5.779 , validation loss: 6.549\n","Epoch: 2262 , train loss: 5.730 , validation loss: 6.552\n","Epoch: 2263 , train loss: 5.799 , validation loss: 6.547\n","Epoch: 2264 , train loss: 5.751 , validation loss: 6.524\n","Epoch: 2265 , train loss: 5.956 , validation loss: 6.468\n","Epoch: 2266 , train loss: 5.783 , validation loss: 6.529\n","Epoch: 2267 , train loss: 5.897 , validation loss: 6.470\n","Epoch: 2268 , train loss: 5.706 , validation loss: 6.547\n","Epoch: 2269 , train loss: 5.783 , validation loss: 6.465\n","Epoch: 2270 , train loss: 5.643 , validation loss: 6.495\n","Epoch: 2271 , train loss: 5.780 , validation loss: 6.485\n","Epoch: 2272 , train loss: 5.809 , validation loss: 6.523\n","Epoch: 2273 , train loss: 5.824 , validation loss: 6.520\n","Epoch: 2274 , train loss: 5.760 , validation loss: 6.559\n","Epoch: 2275 , train loss: 5.839 , validation loss: 6.527\n","Epoch: 2276 , train loss: 5.744 , validation loss: 6.604\n","Epoch: 2277 , train loss: 5.741 , validation loss: 6.548\n","Epoch: 2278 , train loss: 5.765 , validation loss: 6.479\n","Epoch: 2279 , train loss: 5.863 , validation loss: 6.485\n","Epoch: 2280 , train loss: 5.899 , validation loss: 6.508\n","Epoch: 2281 , train loss: 5.829 , validation loss: 6.505\n","Epoch: 2282 , train loss: 5.666 , validation loss: 6.498\n","Epoch: 2283 , train loss: 5.853 , validation loss: 6.503\n","Epoch: 2284 , train loss: 5.773 , validation loss: 6.488\n","Epoch: 2285 , train loss: 5.828 , validation loss: 6.482\n","Epoch: 2286 , train loss: 5.801 , validation loss: 6.533\n","Epoch: 2287 , train loss: 5.800 , validation loss: 6.501\n","Epoch: 2288 , train loss: 5.781 , validation loss: 6.486\n","Epoch: 2289 , train loss: 5.801 , validation loss: 6.452\n","Epoch: 2290 , train loss: 5.706 , validation loss: 6.568\n","Epoch: 2291 , train loss: 5.918 , validation loss: 6.501\n","Epoch: 2292 , train loss: 5.756 , validation loss: 6.489\n","Epoch: 2293 , train loss: 5.810 , validation loss: 6.476\n","Epoch: 2294 , train loss: 5.911 , validation loss: 6.501\n","Epoch: 2295 , train loss: 5.800 , validation loss: 6.496\n","Epoch: 2296 , train loss: 5.753 , validation loss: 6.515\n","Epoch: 2297 , train loss: 5.736 , validation loss: 6.473\n","Epoch: 2298 , train loss: 5.821 , validation loss: 6.522\n","Epoch: 2299 , train loss: 5.861 , validation loss: 6.524\n","Early stopping is trigger at step: 2299 loss:6.418574333190918\n","INFO:tensorflow:Restoring parameters from ./logs/LastVersion/LastVersion.ckpt\n","LastVersion restored.\n","\n","Training complete! Best validation at epoch 1799. Validation loss: 6.419, Test loss: 6.486\n"],"name":"stdout"}]},{"metadata":{"id":"XnOunQQLX2a5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"2c8c0e1c-be3f-4e4d-d8a2-4e59dedcbdb3","executionInfo":{"status":"ok","timestamp":1554669397625,"user_tz":-180,"elapsed":4721,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}}},"cell_type":"code","source":[" restore_model(model_name=\"LastVersion\")"],"execution_count":49,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./logs/LastVersion/LastVersion.ckpt\n","LastVersion restored.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(6.4185743, 6.4857635)"]},"metadata":{"tags":[]},"execution_count":49}]},{"metadata":{"id":"h49yE980hFPe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}